{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Diabetes 30‑Day Readmission Prediction\n",
    "\n",
    " This notebook reproduces the end‑to‑end pipeline described in ChatGPT's analysis:\n",
    "\n",
    " data cleaning, exploratory analysis, feature engineering, class balancing, model training,\n",
    "\n",
    " evaluation, and SHAP interpretation.\n",
    "\n",
    "\n",
    "\n",
    " **Dataset files expected in the working directory:**\n",
    "\n",
    " - `diabetic_data.csv`\n",
    "\n",
    " - `IDS_mapping.csv`\n",
    "\n",
    "\n",
    "\n",
    " Install missing libraries before running (e.g. `xgboost`, `shap`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjay55/code/diabetes-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, confusion_matrix, ConfusionMatrixDisplay)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (101766, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>patient_nbr</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>...</th>\n",
       "      <th>citoglipton</th>\n",
       "      <th>insulin</th>\n",
       "      <th>glyburide-metformin</th>\n",
       "      <th>glipizide-metformin</th>\n",
       "      <th>glimepiride-pioglitazone</th>\n",
       "      <th>metformin-rosiglitazone</th>\n",
       "      <th>metformin-pioglitazone</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>readmitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2278392</td>\n",
       "      <td>8222157</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[0-10)</td>\n",
       "      <td>?</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149190</td>\n",
       "      <td>55629189</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[10-20)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64410</td>\n",
       "      <td>86047875</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Female</td>\n",
       "      <td>[20-30)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500364</td>\n",
       "      <td>82442376</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[30-40)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16680</td>\n",
       "      <td>42519267</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   encounter_id  patient_nbr             race  gender      age weight  \\\n",
       "0       2278392      8222157        Caucasian  Female   [0-10)      ?   \n",
       "1        149190     55629189        Caucasian  Female  [10-20)      ?   \n",
       "2         64410     86047875  AfricanAmerican  Female  [20-30)      ?   \n",
       "3        500364     82442376        Caucasian    Male  [30-40)      ?   \n",
       "4         16680     42519267        Caucasian    Male  [40-50)      ?   \n",
       "\n",
       "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
       "0                  6                        25                    1   \n",
       "1                  1                         1                    7   \n",
       "2                  1                         1                    7   \n",
       "3                  1                         1                    7   \n",
       "4                  1                         1                    7   \n",
       "\n",
       "   time_in_hospital  ... citoglipton insulin  glyburide-metformin  \\\n",
       "0                 1  ...          No      No                   No   \n",
       "1                 3  ...          No      Up                   No   \n",
       "2                 2  ...          No      No                   No   \n",
       "3                 2  ...          No      Up                   No   \n",
       "4                 1  ...          No  Steady                   No   \n",
       "\n",
       "   glipizide-metformin  glimepiride-pioglitazone  metformin-rosiglitazone  \\\n",
       "0                   No                        No                       No   \n",
       "1                   No                        No                       No   \n",
       "2                   No                        No                       No   \n",
       "3                   No                        No                       No   \n",
       "4                   No                        No                       No   \n",
       "\n",
       "   metformin-pioglitazone  change diabetesMed readmitted  \n",
       "0                      No      No          No         NO  \n",
       "1                      No      Ch         Yes        >30  \n",
       "2                      No      No         Yes         NO  \n",
       "3                      No      Ch         Yes         NO  \n",
       "4                      No      Ch         Yes         NO  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DATA_DIR = Path('/home/arjay55/code/datasets/diabetes+130-us+hospitals+for+years+1999-2008')  # change if files are elsewhere\n",
    "df = pd.read_csv(DATA_DIR / 'diabetic_data.csv')\n",
    "ids_map = pd.read_csv(DATA_DIR / 'IDS_mapping.csv')\n",
    "print(f'Data shape: {df.shape}')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_med_change(x):\n",
    "    \"\"\"\n",
    "    Simple ordinal encoder for medication‐change flags.\n",
    "    \n",
    "    Maps:\n",
    "      \"No\"     → 0.0\n",
    "      \"Down\"   → 1.0\n",
    "      \"Steady\" → 2.0\n",
    "      \"Up\"     → 3.0\n",
    "    \n",
    "    Anything else → np.nan\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"no\":      0.0,\n",
    "        \"down\":    1.0,\n",
    "        \"steady\":  2.0,\n",
    "        \"up\":      3.0,\n",
    "    }\n",
    "    # normalize to lower‐case string, then lookup\n",
    "    return mapping.get(str(x).strip().lower(), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied medication change encoding to 23 columns\n",
      "Sample encoded values:\n",
      "   metformin  repaglinide  nateglinide  chlorpropamide  glimepiride\n",
      "0        0.0          0.0          0.0             0.0          0.0\n",
      "1        0.0          0.0          0.0             0.0          0.0\n",
      "2        0.0          0.0          0.0             0.0          0.0\n",
      "3        0.0          0.0          0.0             0.0          0.0\n",
      "4        0.0          0.0          0.0             0.0          0.0\n"
     ]
    }
   ],
   "source": [
    "# Apply medication change encoding to all medication columns\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', \n",
    "    'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(encode_med_change)\n",
    "\n",
    "print(f\"Applied medication change encoding to {len(medication_cols)} columns\")\n",
    "print(\"Sample encoded values:\")\n",
    "print(df[medication_cols[:5]].head())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop weight (97% missing) and impossible genders\n",
    "df = df[df['gender'] != 'Unknown/Invalid'].copy()\n",
    "df.drop(columns=['weight'], inplace=True)\n",
    "\n",
    "# Replace '?' with 'Unknown'\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "df[categorical_cols] = df[categorical_cols].replace('?', 'Unknown')\n",
    "\n",
    "# Remove encounters with discharge disposition indicating death/hospice\n",
    "hospice_codes = [11, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(hospice_codes)]\n",
    "print('After cleaning:', df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.1 Map admission/disposition/source IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping for admission_type_id only (since that's what we have)\n",
    "def build_mapping_from_df(df_map):\n",
    "    # Remove any rows with NaN values\n",
    "    df_clean = df_map.dropna()\n",
    "    return dict(zip(df_clean['admission_type_id'], df_clean['description']))\n",
    "\n",
    "# Build the admission type mapping\n",
    "admission_type_mapping = build_mapping_from_df(ids_map)\n",
    "\n",
    "original_dtype = df['admission_type_id'].dtype\n",
    "\n",
    "# Create new mapping with converted keys\n",
    "if original_dtype in ['int64', 'int32', 'float64']:\n",
    "    # Convert string keys to numeric\n",
    "    admission_type_mapping_fixed = {\n",
    "        int(k): v for k, v in admission_type_mapping.items() \n",
    "        if k.isdigit()\n",
    "    }\n",
    "else:\n",
    "    # Keep as strings\n",
    "    admission_type_mapping_fixed = admission_type_mapping\n",
    "\n",
    "# Apply the mapping\n",
    "df['admission_type_id'] = df['admission_type_id'].map(admission_type_mapping_fixed).fillna('Other')\n",
    "\n",
    "print(\"After applying mapping with converted keys:\")\n",
    "print(df['admission_type_id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.2 Aggregate ICD‑9 diagnosis codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def diag_category(icd):\n",
    "    try:\n",
    "        icd = str(icd)\n",
    "        code = icd.split('.')[0]  # take 3‑digit root\n",
    "        if code.startswith('V') or code.startswith('E'):\n",
    "            return 'Other'\n",
    "        code = int(code)\n",
    "    except:\n",
    "        return 'Other'\n",
    "    if 390 <= code <= 459 or code == 785 or 460 <= code <= 519 or code == 786 or 520 <= code <= 579 or code == 787 or 250 <= code <= 251:\n",
    "        return f'icd_{code}'  # will result to very sparse categories\n",
    "    if 800 <= code <= 999:\n",
    "        return 'Injury'\n",
    "    if 710 <= code <= 739:\n",
    "        return 'Musculoskeletal'\n",
    "    if 140 <= code <= 239:\n",
    "        return 'Neoplasms'\n",
    "    if 580 <= code <= 629 or code == 788:\n",
    "        return 'Genitourinary'\n",
    "    return 'Other'\n",
    "\n",
    "for col in ['diag_1', 'diag_2', 'diag_3']:\n",
    "    df[f'{col}_cat'] = df[col].apply(diag_category)\n",
    "\n",
    "df.drop(columns=['diag_1','diag_2','diag_3'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(columns=['encounter_id','patient_nbr'], inplace=True, errors='ignore') #??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Train‑test split & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_name(col_name):\n",
    "    \"\"\"Clean column names by removing special characters that XGBoost doesn't allow\"\"\"\n",
    "    return str(col_name).replace('[', '_').replace(']', '_').replace('<', '_lt_').replace('>', '_gt_').replace(',', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = (df['readmitted'] == '<30').astype(int)\n",
    "X = df.drop(columns=['readmitted'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "print('Train size:', X_train.shape, 'Pos rate:', y_train.mean().round(3))\n",
    "print('Test size:', X_test.shape, 'Pos rate:', y_test.mean().round(3))\n",
    "\n",
    "# Fix column names to remove special characters that XGBoost doesn't allow\n",
    "X_train.columns = [clean_column_name(col) for col in X_train.columns]\n",
    "X_test.columns = [clean_column_name(col) for col in X_test.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.1 Balance training set by random oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "maj = train[train['readmitted']==0]\n",
    "minu = train[train['readmitted']==1]\n",
    "minu_upsampled = resample(minu, replace=True, n_samples=len(maj), random_state=42)\n",
    "train_bal = pd.concat([maj, minu_upsampled])\n",
    "X_train_bal = train_bal.drop(columns=['readmitted'])\n",
    "y_train_bal = train_bal['readmitted']\n",
    "print('Balanced class counts:', y_train_bal.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.2 One‑hot encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_feats = X_train_bal.select_dtypes(include='object').columns\n",
    "X_train_bal_enc = pd.get_dummies(X_train_bal, columns=cat_feats, drop_first=True)\n",
    "X_test_enc = pd.get_dummies(X_test, columns=cat_feats, drop_first=True)\n",
    "X_train_bal_enc, X_test_enc = X_train_bal_enc.align(X_test_enc, join='left', axis=1, fill_value=0)\n",
    "\n",
    "## haircut\n",
    "X_train_bal_enc.columns = [clean_column_name(col) for col in X_train_bal_enc.columns]\n",
    "X_test_enc.columns = [clean_column_name(col) for col in X_test_enc.columns]\n",
    "\n",
    "# Scale numeric\n",
    "num_feats = X_train_bal_enc.select_dtypes(include=['int64','float64']).columns\n",
    "scaler = StandardScaler()\n",
    "X_train_bal_enc[num_feats] = scaler.fit_transform(X_train_bal_enc[num_feats])\n",
    "X_test_enc[num_feats] = scaler.transform(X_test_enc[num_feats])\n",
    "\n",
    "print(\"Feature engineering completed. Starting model training...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Initializing models...\")\n",
    "logreg = LogisticRegression(max_iter=1000, solver='lbfgs', random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "xgb = XGBClassifier(n_estimators=100, max_depth=6, eval_metric='logloss',\n",
    "                    use_label_encoder=False, verbosity=0, random_state=42)\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "logreg.fit(X_train_bal_enc, y_train_bal)\n",
    "print(\"Training Random Forest...\")\n",
    "rf.fit(X_train_bal_enc, y_train_bal)\n",
    "print(\"Training XGBoost...\")\n",
    "xgb.fit(X_train_bal_enc, y_train_bal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(name, model):\n",
    "    y_pred = model.predict(X_test_enc)\n",
    "    y_prob = model.predict_proba(X_test_enc)[:,1]\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    print(f\"{name:20} Precision: {prec:.3f} Recall: {rec:.3f} F1: {f1:.3f} ROC-AUC: {auc:.3f}\")\n",
    "    return y_pred\n",
    "\n",
    "preds = {}\n",
    "preds['Logistic'] = eval_model('Logistic Regression', logreg)\n",
    "preds['RandomForest'] = eval_model('Random Forest', rf)\n",
    "preds['XGBoost'] = eval_model('XGBoost', xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space with proper types\n",
    "# rf_param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [5, 10, 15, None],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'max_features': ['sqrt', 'log2', None],\n",
    "#     'bootstrap': [True, False]\n",
    "# }\n",
    "\n",
    "# Install: pip install optuna\n",
    "# import optuna\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Define hyperparameters to optimize\n",
    "#     params = {\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "#         'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "#         'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "#         'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "#         'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "#     }\n",
    "    \n",
    "#     model = RandomForestClassifier(**params, random_state=42)\n",
    "#     scores = cross_val_score(model, X_train_bal_enc, y_train_bal, cv=5, scoring='f1')\n",
    "#     return scores.mean()\n",
    "\n",
    "# # Create study and optimize\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=200, n_jobs=10)\n",
    "\n",
    "# print(f\"Best parameters: {study.best_params}\")\n",
    "# print(f\"Best score: {study.best_value}\")\n",
    "\n",
    "# # Train final model with best parameters\n",
    "# best_rf = RandomForestClassifier(**study.best_params, random_state=42)\n",
    "# best_rf.fit(X_train_bal_enc, y_train_bal)\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Define preprocessing steps\n",
    "def create_preprocessing_pipeline():\n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_features = X_train.select_dtypes(include='object').columns.tolist()\n",
    "    numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor, categorical_features, numerical_features\n",
    "\n",
    "# Create pipeline with proper order: preprocess -> balance -> model (with Optuna parameters)\n",
    "def create_model_pipeline(trial=None):\n",
    "    preprocessor, _, _ = create_preprocessing_pipeline()\n",
    "    \n",
    "    # If trial is provided, optimize hyperparameters\n",
    "    if trial is not None:\n",
    "        # Optuna hyperparameter suggestions for XGBoost\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 15)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "        subsample = trial.suggest_float('subsample', 0.6, 1.0)\n",
    "        colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "        reg_alpha = trial.suggest_float('reg_alpha', 0, 1.0)\n",
    "        reg_lambda = trial.suggest_float('reg_lambda', 0, 1.0)\n",
    "        \n",
    "        classifier = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            eval_metric='logloss',\n",
    "            use_label_encoder=False,\n",
    "            verbosity=0,\n",
    "            random_state=42,\n",
    "            n_jobs=10  # Use all available cores\n",
    "        )\n",
    "    else:\n",
    "        # Use default/best known parameters for XGBoost\n",
    "        classifier = XGBClassifier(\n",
    "            n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            eval_metric='logloss', use_label_encoder=False,\n",
    "            verbosity=0, random_state=42\n",
    "        )\n",
    "    \n",
    "    pipeline = ImbPipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('balancer', RandomOverSampler(random_state=42)),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    # Create pipeline with trial parameters\n",
    "    pipeline = create_model_pipeline(trial)\n",
    "    \n",
    "    # Cross-validation with proper data handling\n",
    "    cv_scores = cross_val_score(\n",
    "        pipeline, X_train, y_train, \n",
    "        cv=4,  # Reduced for faster optimization\n",
    "        scoring='f1',\n",
    "        n_jobs=1  # Reduced to prevent system overload\n",
    "    )\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "# Run Optuna optimization\n",
    "print(\"Starting Optuna hyperparameter optimization...\")\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "# Optimize with progress callback\n",
    "# def callback(study, trial):\n",
    "#     if trial.number % 5 == 0:\n",
    "#         print(f\"Trial {trial.number}: Best value = {study.best_value:.4f}\")\n",
    "\n",
    "study.optimize(\n",
    "    objective, \n",
    "    n_trials=500,\n",
    "    # callbacks=[callback],\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "# Print optimization results\n",
    "print(f\"\\nOptimization completed!\")\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best CV F1 score: {study.best_value:.4f}\")\n",
    "\n",
    "# Create final pipeline with best parameters\n",
    "print(\"\\nTraining final model with best parameters...\")\n",
    "best_pipeline = create_model_pipeline()\n",
    "\n",
    "# Update the classifier with best parameters from Optuna\n",
    "best_pipeline.named_steps['classifier'].set_params(**study.best_params)\n",
    "\n",
    "# Cross-validation with best parameters\n",
    "final_cv_scores = cross_val_score(best_pipeline, X_train, y_train, cv=5, scoring='f1')\n",
    "print(f\"Final CV F1 score: {final_cv_scores.mean():.3f} ± {final_cv_scores.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining final model with best parameters...\")\n",
    "best_pipeline = create_model_pipeline()\n",
    "\n",
    "# Update the classifier with best parameters from Optuna\n",
    "best_pipeline.named_steps['classifier'].set_params(**study.best_params,random_state=42)\n",
    "\n",
    "# Cross-validation with best parameters\n",
    "final_cv_scores = cross_val_score(best_pipeline, X_train, y_train, cv=5, scoring='f1')\n",
    "print(f\"Final CV F1 score: {final_cv_scores.mean():.3f} ± {final_cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model on all training data\n",
    "best_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_pred_test = best_pipeline.predict(X_test)\n",
    "y_prob_test = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test)\n",
    "test_recall = recall_score(y_test, y_pred_test)\n",
    "test_auc = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "print(f\"\\nFinal Test Performance:\")\n",
    "print(f\"F1: {test_f1:.3f}\")\n",
    "print(f\"Precision: {test_precision:.3f}\")\n",
    "print(f\"Recall: {test_recall:.3f}\")\n",
    "print(f\"ROC-AUC: {test_auc:.3f}\")\n",
    "\n",
    "# Save the best model for later use\n",
    "best_rf_optimized = best_pipeline.named_steps['classifier']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Readmission', '30-day Readmission'], \n",
    "            yticklabels=['No Readmission', '30-day Readmission'])\n",
    "plt.title('Confusion Matrix - Optimized Random Forest')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"True Negatives: {cm[0,0]}\")\n",
    "print(f\"False Positives: {cm[0,1]}\")\n",
    "print(f\"False Negatives: {cm[1,0]}\")\n",
    "print(f\"True Positives: {cm[1,1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis for the optimized Random Forest\n",
    "# We need to extract the classifier from the pipeline and use preprocessed data\n",
    "\n",
    "# Get the preprocessed test data by transforming through the pipeline steps\n",
    "X_test_preprocessed = best_pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Get the trained classifier from the pipeline\n",
    "trained_classifier = best_pipeline.named_steps['classifier']\n",
    "\n",
    "# Create SHAP explainer for the Random Forest classifier\n",
    "explainer = shap.TreeExplainer(trained_classifier)\n",
    "\n",
    "# Calculate SHAP values using the preprocessed test data\n",
    "# Note: Using a subset for faster computation\n",
    "sample_size = min(1000, len(X_test_preprocessed))\n",
    "X_test_sample = X_test_preprocessed[:sample_size]\n",
    "\n",
    "print(f\"Computing SHAP values for {sample_size} test samples...\")\n",
    "shap_values = explainer.shap_values(X_test_sample, check_additivity=False)\n",
    "\n",
    "# If binary classification, take the positive class SHAP values\n",
    "if isinstance(shap_values, list) and len(shap_values) == 2:\n",
    "    shap_values = shap_values[1]  # positive class\n",
    "\n",
    "# Get feature names from the preprocessor\n",
    "feature_names = best_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for easier handling\n",
    "X_test_sample_df = pd.DataFrame(X_test_sample, columns=feature_names)\n",
    "\n",
    "# Summary plot (bar)\n",
    "shap.summary_plot(shap_values, X_test_sample_df, plot_type='bar', show=False)\n",
    "plt.title('Mean SHAP Feature Importance - Optimized Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed summary plot\n",
    "shap.summary_plot(shap_values, X_test_sample_df, show=False)\n",
    "plt.title('SHAP Summary Plot - Optimized Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diabetes-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
