{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, accuracy_score,\n",
    "                             roc_auc_score, confusion_matrix, ConfusionMatrixDisplay)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pdb\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 1. Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = Path('/home/arjay55/code/datasets/diabetes+130-us+hospitals+for+years+1999-2008')  # change if files are elsewhere\n",
    "df = pd.read_csv(DATA_DIR / 'diabetic_data.csv')\n",
    "ids_map = pd.read_csv(DATA_DIR / 'IDS_mapping.csv')\n",
    "print(f'Data shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns by data type\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "integer_cols = df.select_dtypes(include=['int64', 'int32']).columns\n",
    "\n",
    "print(\"Categorical/Object columns:\")\n",
    "print(f\"Count: {len(categorical_cols)}\")\n",
    "print(categorical_cols.tolist())\n",
    "\n",
    "print(\"\\nInteger columns:\")\n",
    "print(f\"Count: {len(integer_cols)}\")\n",
    "print(integer_cols.tolist())\n",
    "\n",
    "print(f\"\\nTotal columns analyzed: {len(categorical_cols) + len(integer_cols)}\")\n",
    "print(f\"DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# convert ['encounter_id', 'patient_nbr', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id'] to category\n",
    "categorical_cols = ['encounter_id', 'patient_nbr', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n",
    "df[categorical_cols] = df[categorical_cols].astype('category')\n",
    "\n",
    "categorical_cols_rest = df.select_dtypes(include=['object', 'category']).columns\n",
    "# convert rest of the categorical columns to category\n",
    "df[categorical_cols_rest] = df[categorical_cols_rest].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_med_change(x):\n",
    "    \"\"\"\n",
    "    Simple ordinal encoder for medication‐change flags. No translates to zero as there is no drug. \n",
    "    Down can have the value of 1 as as the probability of relatively lower dosage than is more likely., 2 for steady meaning the drugs are normal,\n",
    "    3 for up as the probability of relatively higher dosage than is more likely.\n",
    "    \n",
    "    Maps:\n",
    "      \"No\"     → 0.0\n",
    "      \"Down\"   → 1.0\n",
    "      \"Steady\" → 2.0\n",
    "      \"Up\"     → 3.0\n",
    "    \n",
    "    Anything else → np.nan\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"no\":      0.0,\n",
    "        \"down\":    1.0,\n",
    "        \"steady\":  2.0,\n",
    "        \"up\":      3.0,\n",
    "    }\n",
    "    # normalize to lower‐case string, then lookup\n",
    "    return mapping.get(str(x).strip().lower(), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply medication change encoding to all medication columns\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', \n",
    "    'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(encode_med_change)\n",
    "\n",
    "print(f\"Applied medication change encoding to {len(medication_cols)} columns\")\n",
    "print(\"Sample encoded values:\")\n",
    "print(df[medication_cols[:5]].head())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop weight as 97% have missing weights and drop impossible genders\n",
    "df = df[df['gender'] != 'Unknown/Invalid'].copy()\n",
    "df.drop(columns=['weight'], inplace=True)\n",
    "freq = df[\"patient_nbr\"].value_counts(normalize=True) # Calculate frequency of each patient. More frequent patients are more likely to have chronic conditions.\n",
    "df[\"patient_freq\"] = df[\"patient_nbr\"].map(freq)\n",
    "\n",
    "# Drop patient_nbr as it is not useful for modeling anymore\n",
    "df.drop(columns=['patient_nbr'], inplace=True)\n",
    "\n",
    "# Replace '?' with 'Unknown'\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "df[categorical_cols] = df[categorical_cols].replace('?', 'Unknown')\n",
    "\n",
    "# Remove encounters with discharge disposition indicating death/hospice\n",
    "hospice_codes = [11, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(hospice_codes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute proportions\n",
    "freq = df['discharge_disposition_id'].value_counts(normalize=True)\n",
    "\n",
    "# 3. Select “major” IDs (≥1% of all records)\n",
    "major_ids = set(freq[freq >= 0.01].index)\n",
    "\n",
    "# 4. Map to reduced categories\n",
    "def bucket_disp(x):\n",
    "    return x if x in major_ids else 'Other'\n",
    "\n",
    "df['disch_reduced'] = df['discharge_disposition_id'].apply(bucket_disp)\n",
    "# df_pt['disch_reduced'] = df_pt['discharge_disposition_id'].apply(bucket_disp)\n",
    "\n",
    "# 5. Drop original column\n",
    "df.drop(columns=['discharge_disposition_id'], inplace=True)\n",
    "# 6. Dummify the reduced column\n",
    "df = pd.get_dummies(df, columns=['disch_reduced'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 2.1 Map admission/disposition/source IDs\n",
    "  * Translates IDs to descriptions for easier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping for admission_type_id only (since that's what we have)\n",
    "def build_mapping_from_df(df_map):\n",
    "    # Remove any rows with NaN values\n",
    "    df_clean = df_map.dropna()\n",
    "    return dict(zip(df_clean['admission_type_id'], df_clean['description']))\n",
    "\n",
    "# Build the admission type mapping\n",
    "admission_type_mapping = build_mapping_from_df(ids_map)\n",
    "\n",
    "original_dtype = df['admission_type_id'].dtype\n",
    "\n",
    "# Create new mapping with converted keys\n",
    "if original_dtype in ['int64', 'int32', 'float64', 'category']:\n",
    "    # Convert string keys to numeric\n",
    "    admission_type_mapping_fixed = {\n",
    "        int(k): v for k, v in admission_type_mapping.items() \n",
    "        if k.isdigit()\n",
    "    }\n",
    "else:\n",
    "    # Keep as strings\n",
    "    admission_type_mapping_fixed = admission_type_mapping\n",
    "\n",
    "# Apply the mapping\n",
    "df['admission_type_id'] = df['admission_type_id'].map(admission_type_mapping_fixed)\n",
    "# df_pt['admission_type_id'] = df_pt['admission_type_id'].map(admission_type_mapping_fixed).fillna('Other')\n",
    "print(\"After applying mapping with converted keys:\")\n",
    "print(df['admission_type_id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 2.2 Aggregate ICD‑9 diagnosis codes\n",
    "* First if statement are focused on internal, coronary and diabetic diseases, which could have comorbidities with each other, and thus we choose to make this detailed.\n",
    "* Other diseases are grouped, as they can have of less influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def diag_category(icd):\n",
    "    try:\n",
    "        icd = str(icd)\n",
    "        code = icd.split('.')[0]  # take 3‑digit root\n",
    "        if code.startswith('V') or code.startswith('E'):\n",
    "            return 'Other'\n",
    "        code = int(code)\n",
    "    except:\n",
    "        return 'Other'\n",
    "    if 390 <= code <= 459 or code == 785 or 460 <= code <= 519 or code == 786 or 520 <= code <= 579 or code == 787 or 250 <= code <= 251:\n",
    "        return f'icd_{code}'  # will result to very sparse categories\n",
    "    if 800 <= code <= 999:\n",
    "        return 'Injury'\n",
    "    if 710 <= code <= 739:\n",
    "        return 'Musculoskeletal'\n",
    "    if 140 <= code <= 239:\n",
    "        return 'Neoplasms'\n",
    "    if 580 <= code <= 629 or code == 788:\n",
    "        return 'Genitourinary'\n",
    "    return 'Other'\n",
    "\n",
    "for col in ['diag_1', 'diag_2', 'diag_3']:\n",
    "    df[f'{col}_cat'] = df[col].apply(diag_category)\n",
    "    # df_pt[f'{col}_cat'] = df_pt[col].apply(diag_category)\n",
    "\n",
    "df.drop(columns=['diag_1','diag_2','diag_3'], inplace=True)\n",
    "# df_pt.drop(columns=['diag_1','diag_2','diag_3'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encounter_id has no relevance in the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(columns=['encounter_id'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt = df.copy() # for Pytorch Tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 3. Train‑test split & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_name(col_name):\n",
    "    \"\"\"Clean column names by removing special characters that XGBoost doesn't allow\"\"\"\n",
    "    return str(col_name).replace('[', '_').replace(']', '_').replace('<', '_lt_').replace('>', '_gt_').replace(',', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = (df['readmitted'] == '<30').astype(int)\n",
    "X = df.drop(columns=['readmitted'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( #stratified sampling\n",
    "    X, y, test_size=0.3, stratify=y, random_state=1803)\n",
    "\n",
    "print('Train size:', X_train.shape, 'Pos rate:', y_train.mean().round(3))\n",
    "print('Test size:', X_test.shape, 'Pos rate:', y_test.mean().round(3))\n",
    "\n",
    "# Fix column names to remove special characters that XGBoost doesn't allow\n",
    "X_train.columns = [clean_column_name(col) for col in X_train.columns]\n",
    "X_test.columns = [clean_column_name(col) for col in X_test.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 3.1 Balance training set by random oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "maj = train[train['readmitted']==0]\n",
    "minu = train[train['readmitted']==1]\n",
    "minu_upsampled = resample(minu, replace=True, n_samples=len(maj), random_state=1803)\n",
    "train_bal = pd.concat([maj, minu_upsampled])\n",
    "X_train_bal = train_bal.drop(columns=['readmitted'])\n",
    "y_train_bal = train_bal['readmitted']\n",
    "print('Balanced class counts:', y_train_bal.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 3.2 One‑hot encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# categorical features (\"object\" dtype) are dummified, meaning they are converted to one-hot encoded columns.\n",
    "cat_feats = X_train_bal.select_dtypes(include=['object','category']).columns\n",
    "X_train_bal_enc = pd.get_dummies(X_train_bal, columns=cat_feats, drop_first=True) # reduce collinearity\n",
    "X_test_enc = pd.get_dummies(X_test, columns=cat_feats, drop_first=True) # reduce collinearity\n",
    "X_train_bal_enc, X_test_enc = X_train_bal_enc.align(X_test_enc, join='left', axis=1, fill_value=0)\n",
    "\n",
    "## haircut for it to be compatible with XGBoost\n",
    "X_train_bal_enc.columns = [clean_column_name(col) for col in X_train_bal_enc.columns]\n",
    "X_test_enc.columns = [clean_column_name(col) for col in X_test_enc.columns]\n",
    "\n",
    "# Apply standard scaling to numeric features\n",
    "num_feats = X_train_bal_enc.select_dtypes(include=['int64','float64']).columns\n",
    "scaler = StandardScaler()\n",
    "X_train_bal_enc[num_feats] = scaler.fit_transform(X_train_bal_enc[num_feats])\n",
    "X_test_enc[num_feats] = scaler.transform(X_test_enc[num_feats])\n",
    "\n",
    "print(\"Feature engineering for baseline runs completed.\")\n",
    "# Dummify categorical variables for X_train and X_test\n",
    "\n",
    "print(\"Creating dummy variables for training and test sets for pipeline use...\")\n",
    "\n",
    "# Get categorical columns\n",
    "cat_cols = X_train.select_dtypes(include=['object','category']).columns\n",
    "print(f\"Categorical columns to encode: {list(cat_cols)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Dummify X_train and X_test\n",
    "X_train = pd.get_dummies(X_train, columns=cat_feats, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=cat_feats, drop_first=True)\n",
    "\n",
    "scaler = StandardScaler() # normalize!\n",
    "X_train[num_feats] = scaler.fit_transform(X_train[num_feats])\n",
    "X_test[num_feats] = scaler.transform(X_test[num_feats])\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Clean column names for XGBoost compatibility\n",
    "X_train.columns = [clean_column_name(col) for col in X_train.columns]\n",
    "X_test.columns = [clean_column_name(col) for col in X_test.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Initializing models...\")\n",
    "logreg = LogisticRegression(max_iter=1000, solver='lbfgs', random_state=1803)\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=1803)\n",
    "xgb = XGBClassifier(n_estimators=100, max_depth=6, eval_metric='logloss',\n",
    "                    use_label_encoder=False, verbosity=0, random_state=1803)\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "logreg.fit(X_train_bal_enc, y_train_bal)\n",
    "print(\"Training Random Forest...\")\n",
    "rf.fit(X_train_bal_enc, y_train_bal)\n",
    "print(\"Training XGBoost...\")\n",
    "xgb.fit(X_train_bal_enc, y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(name, model):\n",
    "    y_pred = model.predict(X_test_enc)\n",
    "    y_prob = model.predict_proba(X_test_enc)[:,1]\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name:20} Precision: {prec:.3f} Recall: {rec:.3f} F1: {f1:.3f} ROC-AUC: {auc:.3f} Accuracy: {acc:.3f}\")\n",
    "    return y_pred\n",
    "\n",
    "preds = {}\n",
    "preds['Logistic'] = eval_model('Logistic Regression', logreg)\n",
    "preds['RandomForest'] = eval_model('Random Forest', rf)\n",
    "preds['XGBoost'] = eval_model('XGBoost', xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Results show suboptimal performance. The class imbalance is significant, due to small positivity rate of 0.113.\n",
    "* We will proceed with XGBOOST due to its versatility and a go-to algorithm for tabular data.\n",
    "* We will use optuna as a hyperparameter tuning tool, a generic hyperparameter tuning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "\n",
    "# Create pipeline with proper order: preprocess -> balance -> model (with Optuna parameters)\n",
    "def create_model_pipeline(trial=None):\n",
    "    \n",
    "    # If trial is provided, optimize hyperparameters\n",
    "    if trial is not None:\n",
    "        # Optuna hyperparameter suggestions for XGBoost\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 15)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "        subsample = trial.suggest_float('subsample', 0.6, 1.0)\n",
    "        colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "        reg_alpha = trial.suggest_float('reg_alpha', 0, 1.0)\n",
    "        reg_lambda = trial.suggest_float('reg_lambda', 0, 1.0)\n",
    "        \n",
    "        classifier = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            eval_metric='logloss',\n",
    "            use_label_encoder=False,\n",
    "            verbosity=0,\n",
    "            random_state=1803,\n",
    "            n_jobs=10  # Use all available cores\n",
    "        )\n",
    "    else:\n",
    "        # Use default/best known parameters for XGBoost\n",
    "        classifier = XGBClassifier(\n",
    "            n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            eval_metric='logloss', use_label_encoder=False,\n",
    "            verbosity=0, random_state=1803\n",
    "        )\n",
    "    rng = np.random.default_rng()\n",
    "    pipeline = ImbPipeline([ \n",
    "        ('balancer', SMOTE(random_state=rng.integers(2**16))), # pipeline performs oversampling per each fold, avoiding data leakage\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    # Create pipeline with trial parameters\n",
    "    pipeline = create_model_pipeline(trial)\n",
    "    \n",
    "    # Cross-validation with proper data handling\n",
    "\n",
    "    cv_scores = cross_val_score(\n",
    "        pipeline, X_train, y_train, \n",
    "        cv=4,  \n",
    "        scoring='accuracy',\n",
    "        n_jobs=1  # Reduced to prevent system overload\n",
    "    )\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "# Run Optuna optimization\n",
    "print(\"Starting Optuna hyperparameter optimization...\")\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=1803)\n",
    ")\n",
    "\n",
    "# Optimize with progress callback\n",
    "# def callback(study, trial):\n",
    "#     if trial.number % 5 == 0:\n",
    "#         print(f\"Trial {trial.number}: Best value = {study.best_value:.4f}\")\n",
    "\n",
    "study.optimize(\n",
    "    objective, \n",
    "    n_trials=100,\n",
    "    # callbacks=[callback],\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "# Print optimization results\n",
    "print(f\"\\nOptimization completed!\")\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best CV accuracy score: {study.best_value:.4f}\")\n",
    "\n",
    "# Create final pipeline with best parameters\n",
    "print(\"\\nTraining final model with best parameters...\")\n",
    "best_pipeline = create_model_pipeline()\n",
    "\n",
    "# Update the classifier with best parameters from Optuna\n",
    "best_pipeline.named_steps['classifier'].set_params(**study.best_params)\n",
    "\n",
    "# Cross-validation with best parameters\n",
    "final_cv_scores = cross_val_score(best_pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Final CV accuracy score: {final_cv_scores.mean():.3f} ± {final_cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining final model with best parameters...\")\n",
    "best_pipeline = create_model_pipeline()\n",
    "\n",
    "# Update the classifier with best parameters from Optuna\n",
    "best_pipeline.named_steps['classifier'].set_params(**study.best_params,random_state=1803)\n",
    "\n",
    "# Cross-validation with best parameters\n",
    "final_cv_scores = cross_val_score(best_pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Final CV accuracy score: {final_cv_scores.mean():.3f} ± {final_cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model on all training data\n",
    "best_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_pred_test = best_pipeline.predict(X_test)\n",
    "y_prob_test = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test)\n",
    "test_recall = recall_score(y_test, y_pred_test)\n",
    "test_auc = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "print(f\"\\nFinal Test Performance:\")\n",
    "print(f\"Accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"F1: {test_f1:.3f}\")\n",
    "print(f\"Precision: {test_precision:.3f}\")\n",
    "print(f\"Recall: {test_recall:.3f}\")\n",
    "print(f\"ROC-AUC: {test_auc:.3f}\")\n",
    "\n",
    "# Save the best model for later use\n",
    "best_rf_optimized = best_pipeline.named_steps['classifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy was well achieved. However the lack of data for readmissions resulted in highly skewed result plus some other modeling imperfections. The model is not yet safe for deployment. A higher recall is better. Class weighting that biases on readmission rates will be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Readmission', '30-day Readmission'], \n",
    "            yticklabels=['No Readmission', '30-day Readmission'])\n",
    "plt.title('Confusion Matrix - Optimized Random Forest')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"True Negatives: {cm[0,0]}\")\n",
    "print(f\"False Positives: {cm[0,1]}\")\n",
    "print(f\"False Negatives: {cm[1,0]}\")\n",
    "print(f\"True Positives: {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tabular Implementation\n",
    "\n",
    "Now we'll implement the same training pipeline using PyTorch Tabular with neural networks instead of XGBoost. Pytorch Tabular aims to implement suitable neural network architectures for tabular data with ease of use in using other popular frameworks, like Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch Tabular\n",
    "import torch\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"PyTorch Tabular imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch Tabular\n",
    "# We'll use the same train/test split as XGBoost but with different preprocessing\n",
    "print(\"Preparing data for PyTorch Tabular...\")\n",
    "\n",
    "# Start with the original X_train, X_test, y_train, y_test\n",
    "# Reset from the original data before one-hot encoding\n",
    "y = (df_pt['readmitted'] == '<30').astype(int)\n",
    "X = df_pt.drop(columns=['readmitted'])\n",
    "\n",
    "X_train_pt, X_test_pt, y_train_pt, y_test_pt = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=1803)\n",
    "\n",
    "print('PyTorch Tabular - Train size:', X_train_pt.shape, 'Pos rate:', y_train_pt.mean().round(3))\n",
    "print('PyTorch Tabular - Test size:', X_test_pt.shape, 'Pos rate:', y_test_pt.mean().round(3))\n",
    "\n",
    "# Clean column names\n",
    "X_train_pt.columns = [clean_column_name(col) for col in X_train_pt.columns]\n",
    "X_test_pt.columns = [clean_column_name(col) for col in X_test_pt.columns]\n",
    "\n",
    "train_df_pt = X_train_pt.copy()\n",
    "train_df_pt[\"target\"] = y_train_pt.values\n",
    "print(\"Data prepared for PyTorch Tabular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance training set by random oversampling (same as XGBoost)\n",
    "print(\"Balancing training data...\")\n",
    "\n",
    "maj_pt = train_df_pt[train_df_pt['target']==0]\n",
    "minu_pt = train_df_pt[train_df_pt['target']==1]\n",
    "minu_upsampled_pt = resample(minu_pt, replace=True, n_samples=len(maj_pt), random_state=1803)\n",
    "train_bal_df_pt = pd.concat([maj_pt, minu_upsampled_pt])\n",
    "\n",
    "print('Balanced class counts for PyTorch Tabular:', train_bal_df_pt['target'].value_counts())\n",
    "print('Balanced training set shape:', train_bal_df_pt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and numerical columns for PyTorch Tabular\n",
    "categorical_cols_pt = [col for col in X_train_pt.columns if X_train_pt[col].dtype == 'object' or X_train_pt[col].dtype.name == 'category']\n",
    "numerical_cols_pt = [col for col in X_train_pt.columns if X_train_pt[col].dtype != 'object' and X_train_pt[col].dtype.name != 'category']\n",
    "\n",
    "print(f\"Categorical columns ({len(categorical_cols_pt)}): {categorical_cols_pt[:5]}...\")\n",
    "print(f\"Numerical columns ({len(numerical_cols_pt)}): {numerical_cols_pt[:5]}...\")\n",
    "\n",
    "# Make sure we have the correct categoricals\n",
    "categorical_cols_pt = []\n",
    "numerical_cols_pt = []\n",
    "\n",
    "for col in X_train_pt.columns:\n",
    "    if X_train_pt[col].dtype == 'object' or str(X_train_pt[col].dtype) == 'category':\n",
    "        categorical_cols_pt.append(col)\n",
    "    else:\n",
    "        numerical_cols_pt.append(col)\n",
    "\n",
    "# Dummify categorical columns for PyTorch Tabular\n",
    "# X_train_pt = pd.get_dummies(X_train_pt, columns=categorical_cols_pt, drop_first=True)\n",
    "# X_test_pt = pd.get_dummies(X_test_pt, columns=categorical_cols_pt, drop_first=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Normalize numerical columns for PyTorch Tabular\n",
    "X_train_pt[numerical_cols_pt] = scaler.fit_transform(X_train_pt[numerical_cols_pt])\n",
    "X_test_pt[numerical_cols_pt] = scaler.transform(X_test_pt[numerical_cols_pt])\n",
    "X_train_pt, X_test_pt = X_train_pt.align(X_test_pt, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Add target column to create complete datasets\n",
    "train_df_pt = X_train_pt.copy()\n",
    "train_df_pt['target'] = y_train_pt.values\n",
    "\n",
    "test_df_pt = X_test_pt.copy()\n",
    "test_df_pt['target'] = y_test_pt.values\n",
    "\n",
    "# Configure PyTorch Tabular Data Config\n",
    "data_config = DataConfig(\n",
    "    target=['target'],  # Target column\n",
    "    continuous_cols=numerical_cols_pt,  # Numerical columns\n",
    "    categorical_cols=categorical_cols_pt,  # Categorical columns\n",
    "    normalize_continuous_features=True,  # Similar to StandardScaler\n",
    ")\n",
    "\n",
    "print(\"PyTorch Tabular Data Config created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will use CategoryEmbeddingModelConfig, where categorical data are transformed into high dimensional embeddings.\n",
    "* We will go through the process of trying the model incrementally prior to proceeding to hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline PyTorch Tabular model configuration\n",
    "model_config = CategoryEmbeddingModelConfig(\n",
    "    task=\"classification\",\n",
    "    layers=\"128-64-32\",  # Neural network architecture\n",
    "    activation=\"ReLU\",\n",
    "    dropout=0.1,\n",
    "    use_batch_norm=True,  # Correct parameter name\n",
    "    learning_rate=1e-3,\n",
    "    seed=1803,\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024,\n",
    "    max_epochs=50,\n",
    "    early_stopping=\"valid_loss\",\n",
    "    early_stopping_patience=10,\n",
    "    checkpoints=None,  # Disable checkpoints to avoid loading issues\n",
    "    load_best=False,   # Don't try to load best model\n",
    "    progress_bar=\"none\",  # Disable progress bar for cleaner output\n",
    "    auto_lr_find=False,  # We'll set learning rate manually\n",
    "    auto_select_gpus=torch.cuda.is_available(),\n",
    "    seed=1803,\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "print(\"PyTorch Tabular configurations created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline PyTorch Tabular model\n",
    "print(\"Training baseline PyTorch Tabular model...\")\n",
    "\n",
    "baseline_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "baseline_model.fit(train=train_bal_df_pt, validation=test_df_pt)\n",
    "\n",
    "print(\"Baseline PyTorch Tabular model training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline PyTorch Tabular model\n",
    "print(\"Evaluating baseline PyTorch Tabular model...\")\n",
    "\n",
    "# Make predictions\n",
    "baseline_pred_proba = baseline_model.predict(test_df_pt)\n",
    "print(\"Prediction output shape:\", baseline_pred_proba.shape)\n",
    "print(\"Prediction output columns:\", baseline_pred_proba.columns.tolist())\n",
    "\n",
    "# Get prediction probabilities - use the correct column name\n",
    "if '1' in baseline_pred_proba.columns:\n",
    "    baseline_proba_values = baseline_pred_proba['1'].values\n",
    "elif '1_probability' in baseline_pred_proba.columns:\n",
    "    baseline_proba_values = baseline_pred_proba['1_probability'].values\n",
    "else:\n",
    "    # Try the first numeric column after the identifier columns\n",
    "    prob_cols = [col for col in baseline_pred_proba.columns if col not in ['patient_nbr', 'target']]\n",
    "    baseline_proba_values = baseline_pred_proba[prob_cols[0]].values\n",
    "\n",
    "baseline_pred = (baseline_proba_values > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "baseline_prec = precision_score(y_test_pt, baseline_pred)\n",
    "baseline_rec = recall_score(y_test_pt, baseline_pred)\n",
    "baseline_f1 = f1_score(y_test_pt, baseline_pred)\n",
    "baseline_auc = roc_auc_score(y_test_pt, baseline_proba_values)\n",
    "\n",
    "print(f\"Baseline PyTorch Tabular Performance:\")\n",
    "print(f\"Precision: {baseline_prec:.3f}\")\n",
    "print(f\"Recall: {baseline_rec:.3f}\")\n",
    "print(f\"F1: {baseline_f1:.3f}\")\n",
    "print(f\"ROC-AUC: {baseline_auc:.3f}\")\n",
    "print (f\"Accuracy: {accuracy_score(y_test_pt, baseline_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Results are suboptimal, will proceed to k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement 4-fold cross-validation for PyTorch Tabular\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def pytorch_tabular_cv(X_data, y_data, n_folds=4, model_params=None):\n",
    "    \"\"\"\n",
    "    Perform cross-validation for PyTorch Tabular model\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=1803)\n",
    "    cv_scores = []\n",
    "    \n",
    "    # Combine X and y for easier handling\n",
    "    full_data = X_data.copy()\n",
    "    full_data['target'] = y_data.values\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_data, y_data)):\n",
    "        print(f\"Training fold {fold + 1}/{n_folds}...\")\n",
    "        \n",
    "        # Split data\n",
    "        train_fold = full_data.iloc[train_idx]\n",
    "        val_fold = full_data.iloc[val_idx]\n",
    "        \n",
    "        # Balance training fold\n",
    "        maj_fold = train_fold[train_fold['target']==0]\n",
    "        minu_fold = train_fold[train_fold['target']==1]\n",
    "        minu_upsampled_fold = resample(minu_fold, replace=True, n_samples=len(maj_fold), random_state=1803+fold)\n",
    "        train_fold_balanced = pd.concat([maj_fold, minu_upsampled_fold])\n",
    "        \n",
    "        # Create model configuration\n",
    "        if model_params is None:\n",
    "            fold_model_config = CategoryEmbeddingModelConfig(\n",
    "                task=\"classification\",\n",
    "                layers=\"128-64-32\",\n",
    "                activation=\"ReLU\", \n",
    "                dropout=0.1,\n",
    "                use_batch_norm=True,  # Fixed parameter name\n",
    "                learning_rate=1e-3,\n",
    "                seed=1803+fold,\n",
    "            )\n",
    "        else:\n",
    "            fold_model_config = CategoryEmbeddingModelConfig(\n",
    "                task=\"classification\",\n",
    "                **model_params,\n",
    "                seed=1803+fold,\n",
    "            )\n",
    "        \n",
    "        fold_trainer_config = TrainerConfig(\n",
    "            batch_size=1024,\n",
    "            max_epochs=30,  # Reduced for CV\n",
    "            early_stopping=\"valid_loss\",\n",
    "            early_stopping_patience=5,\n",
    "            checkpoints=None,  # Don't save checkpoints for CV\n",
    "            load_best=True,   # Don't try to load best model\n",
    "            progress_bar=\"none\",\n",
    "            auto_lr_find=False,\n",
    "            auto_select_gpus=torch.cuda.is_available(),\n",
    "            seed=1803+fold,\n",
    "        )\n",
    "        \n",
    "        # Create and train model\n",
    "        fold_model = TabularModel(\n",
    "            data_config=data_config,\n",
    "            model_config=fold_model_config,\n",
    "            optimizer_config=optimizer_config,\n",
    "            trainer_config=fold_trainer_config,\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            fold_model.fit(train=train_fold_balanced, validation=val_fold)\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            val_pred_proba = fold_model.predict(val_fold)\n",
    "            # Use correct column name for predictions\n",
    "            val_proba_values = val_pred_proba['target_1_probability'].values\n",
    "            val_pred = (val_proba_values > 0.5).astype(int)\n",
    "            val_accuracy = accuracy_score(val_fold['target'], val_pred)\n",
    "            cv_scores.append(val_accuracy)\n",
    "            \n",
    "            print(f\"Fold {fold + 1} accuracy score: {val_accuracy:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fold {fold + 1}: {e}\")\n",
    "            cv_scores.append(0.0)  # Add poor score for failed fold\n",
    "    \n",
    "    return cv_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform 4-fold cross-validation with baseline model\n",
    "print(\"Performing 4-fold cross-validation with PyTorch Tabular...\")\n",
    "cv_scores_pt = pytorch_tabular_cv(X_train_pt, y_train_pt, n_folds=4)\n",
    "\n",
    "print(f\"\\nPyTorch Tabular CV Results:\")\n",
    "print(f\"Mean Accuracy: {np.mean(cv_scores_pt):.3f} ± {np.std(cv_scores_pt):.3f}\")\n",
    "print(f\"Individual fold scores: {[f'{score:.3f}' for score in cv_scores_pt]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracies are suboptimal. Hopefully hyperparameter tuning will enhance this.\n",
    "* In this case, layer depths, activation, batch sizes, etc will be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for PyTorch Tabular using Optuna\n",
    "import optuna\n",
    "\n",
    "def pytorch_tabular_objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for PyTorch Tabular hyperparameter optimization\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    layers_depth = trial.suggest_int('layers_depth', 2, 3, 4, 5)\n",
    "    layer_size = trial.suggest_categorical('layer_size', [64, 128, 256, 512])\n",
    "    \n",
    "    # Create layer string\n",
    "    if layers_depth == 2:\n",
    "        layers = f\"{layer_size}-{layer_size//2}\"\n",
    "    elif layers_depth == 3:\n",
    "        layers = f\"{layer_size}-{layer_size//2}-{layer_size//4}\"\n",
    "    else:  # layers_depth == 4\n",
    "        layers = f\"{layer_size}-{layer_size//2}-{layer_size//4}-{layer_size//8}\"\n",
    "    \n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [512, 1024, 2048])\n",
    "    activation = trial.suggest_categorical('activation', ['ReLU', 'GELU', 'LeakyReLU'])\n",
    "    \n",
    "    # Model parameters\n",
    "    model_params = {\n",
    "        'layers': layers,\n",
    "        'activation': activation,\n",
    "        'dropout': dropout,\n",
    "        'use_batch_norm': True,  # Fixed parameter name\n",
    "        'learning_rate': learning_rate,\n",
    "    }\n",
    "    \n",
    "    # Use trainer config with suggested batch size\n",
    "    global data_config, optimizer_config\n",
    "    \n",
    "    # Perform cross-validation with reduced folds for speed\n",
    "    cv_scores = []\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1803)  # Reduced to 3 folds for speed\n",
    "    \n",
    "    # Combine X and y for easier handling\n",
    "    full_data = X_train_pt.copy()\n",
    "    full_data['target'] = y_train_pt.values\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_pt, y_train_pt)):\n",
    "        # Split data\n",
    "        train_fold = full_data.iloc[train_idx]\n",
    "        val_fold = full_data.iloc[val_idx]\n",
    "        \n",
    "        # Balance training fold\n",
    "        maj_fold = train_fold[train_fold['target']==0]\n",
    "        minu_fold = train_fold[train_fold['target']==1]\n",
    "        minu_upsampled_fold = resample(minu_fold, replace=True, n_samples=len(maj_fold))\n",
    "        train_fold_balanced = pd.concat([maj_fold, minu_upsampled_fold])\n",
    "        \n",
    "        # Create model configuration\n",
    "        fold_model_config = CategoryEmbeddingModelConfig(\n",
    "            task=\"classification\",\n",
    "            **model_params,\n",
    "            seed=1803+fold,\n",
    "        )\n",
    "        \n",
    "        fold_trainer_config = TrainerConfig(\n",
    "            batch_size=batch_size,\n",
    "            max_epochs=20,  # Reduced for hyperparameter tuning\n",
    "            early_stopping=\"valid_loss\",\n",
    "            early_stopping_patience=3,\n",
    "            checkpoints=None,\n",
    "            load_best=False,\n",
    "            progress_bar=\"none\",\n",
    "            auto_lr_find=False,\n",
    "            auto_select_gpus=torch.cuda.is_available(),\n",
    "            seed=1803+fold,\n",
    "        )\n",
    "        \n",
    "        # Create and train model\n",
    "        fold_model = TabularModel(\n",
    "            data_config=data_config,\n",
    "            model_config=fold_model_config,\n",
    "            optimizer_config=optimizer_config,\n",
    "            trainer_config=fold_trainer_config,\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            fold_model.fit(train=train_fold_balanced, validation=val_fold)\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            val_pred_proba = fold_model.predict(val_fold)\n",
    "            val_proba_values = val_pred_proba['target_1_probability'].values\n",
    "            val_pred = (val_proba_values > 0.5).astype(int)\n",
    "            val_accuracy = accuracy_score(val_fold['target'], val_pred)\n",
    "            cv_scores.append(val_accuracy)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fold {fold}: {e}\")\n",
    "            return 0.0  # Return poor score for failed trials\n",
    "    \n",
    "    if len(cv_scores) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Run Optuna optimization for PyTorch Tabular (smaller trial for speed)\n",
    "print(\"Starting Optuna hyperparameter optimization for PyTorch Tabular...\")\n",
    "study_pt = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=1803)\n",
    ")\n",
    "\n",
    "study_pt.optimize(\n",
    "    pytorch_tabular_objective, \n",
    "    n_trials=100,  \n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "# Print optimization results\n",
    "print(f\"\\nPyTorch Tabular Optimization completed!\")\n",
    "print(f\"Best parameters: {study_pt.best_params}\")\n",
    "print(f\"Best CV accuracy score: {study_pt.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final optimized PyTorch Tabular model\n",
    "print(\"Training final optimized PyTorch Tabular model...\")\n",
    "\n",
    "# Extract best parameters\n",
    "best_params_pt = study_pt.best_params\n",
    "layers_depth = best_params_pt['layers_depth']\n",
    "layer_size = best_params_pt['layer_size']\n",
    "\n",
    "# Create layer string\n",
    "if layers_depth == 2:\n",
    "    layers = f\"{layer_size}-{layer_size//2}\"\n",
    "elif layers_depth == 3:\n",
    "    layers = f\"{layer_size}-{layer_size//2}-{layer_size//4}\"\n",
    "else:  # layers_depth == 4\n",
    "    layers = f\"{layer_size}-{layer_size//2}-{layer_size//4}-{layer_size//8}\"\n",
    "\n",
    "# Create final model configuration with best parameters\n",
    "final_model_config_pt = CategoryEmbeddingModelConfig(\n",
    "    task=\"classification\",\n",
    "    layers=layers,\n",
    "    activation=best_params_pt['activation'],\n",
    "    dropout=best_params_pt['dropout'],\n",
    "    use_batch_norm=True,\n",
    "    learning_rate=best_params_pt['learning_rate'],\n",
    "    seed=1803,\n",
    ")\n",
    "\n",
    "final_trainer_config_pt = TrainerConfig(\n",
    "    batch_size=best_params_pt['batch_size'],\n",
    "    max_epochs=50,  # Full epochs for final model\n",
    "    early_stopping=\"valid_loss\",\n",
    "    early_stopping_patience=5,\n",
    "    checkpoints=\"valid_loss\",\n",
    "    load_best=False,\n",
    "    progress_bar=\"none\",\n",
    "    auto_lr_find=False,\n",
    "    auto_select_gpus=torch.cuda.is_available(),\n",
    "    seed=1803,\n",
    ")\n",
    "\n",
    "# Create final model\n",
    "final_model_pt = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=final_model_config_pt,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=final_trainer_config_pt,\n",
    ")\n",
    "\n",
    "# Train final model\n",
    "final_model_pt.fit(train=train_bal_df_pt, validation=test_df_pt)\n",
    "\n",
    "print(\"Final PyTorch Tabular model training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cross-validation with optimized parameters\n",
    "# print(\"Performing final 4-fold cross-validation with optimized parameters...\")\n",
    "\n",
    "# # Extract best parameters for CV function\n",
    "# best_model_params = {\n",
    "#     'layers': layers,\n",
    "#     'activation': best_params_pt['activation'],\n",
    "#     'dropout': best_params_pt['dropout'],\n",
    "#     'use_batch_norm': True,\n",
    "#     'learning_rate': best_params_pt['learning_rate'],\n",
    "# }\n",
    "\n",
    "# # Perform final CV with optimized parameters\n",
    "# final_cv_scores_pt = pytorch_tabular_cv(X_train_pt, y_train_pt, n_folds=4, model_params=best_model_params)\n",
    "\n",
    "# print(f\"\\nFinal PyTorch Tabular CV Results (with optimization):\")\n",
    "# print(f\"Mean Accuracy: {np.mean(final_cv_scores_pt):.3f} ± {np.std(final_cv_scores_pt):.3f}\")\n",
    "# print(f\"Individual fold scores: {[f'{score:.3f}' for score in final_cv_scores_pt]}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating final model on test set...\")\n",
    "final_pred_proba_pt = final_model_pt.predict(test_df_pt)\n",
    "final_pred_pt = (final_pred_proba_pt['target_1_probability'].values > 0.5).astype(int)\n",
    "final_accuracy_pt = accuracy_score(y_test_pt, final_pred_pt)\n",
    "final_prec_pt = precision_score(y_test_pt, final_pred_pt)\n",
    "final_rec_pt = recall_score(y_test_pt, final_pred_pt)\n",
    "final_f1_pt = f1_score(y_test_pt, final_pred_pt)\n",
    "final_auc_pt = roc_auc_score(y_test_pt, final_pred_proba_pt['target_1_probability'].values)\n",
    "\n",
    "print(f\"\\nFinal PyTorch Tabular Test Performance:\")\n",
    "print(f\"Accuracy: {final_accuracy_pt:.3f}\")\n",
    "print(f\"Precision: {final_prec_pt:.3f}\")\n",
    "print(f\"Recall: {final_rec_pt:.3f}\")\n",
    "print(f\"F1: {final_f1_pt:.3f}\")\n",
    "print(f\"ROC-AUC: {final_auc_pt:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final PyTorch Tabular model\n",
    "print(\"Saving PyTorch Tabular model...\")\n",
    "\n",
    "# Save the model\n",
    "model_save_path = \"best_pytorch_tabular_model\"\n",
    "final_model_pt.save_model(model_save_path)\n",
    "\n",
    "# Also save the best hyperparameters\n",
    "with open(\"best_pytorch_tabular_params.txt\", \"w\") as f:\n",
    "    f.write(\"Best PyTorch Tabular Hyperparameters:\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\")\n",
    "    for key, value in study_pt.best_params.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "    f.write(f\"\\nBest CV Accuracy Score: {study_pt.best_value:.4f}\\n\")\n",
    "    f.write(f\"Final Test Accuracy Score: {final_accuracy_pt:.4f}\\n\")\n",
    "    f.write(f\"Final Test F1 Score: {final_f1_pt:.4f}\\n\")\n",
    "    f.write(f\"Final Test ROC-AUC: {final_auc_pt:.4f}\\n\")\n",
    "\n",
    "print(\"Model and parameters saved successfully\")\n",
    "\n",
    "# Create confusion matrix for PyTorch Tabular\n",
    "cm_pt = confusion_matrix(y_test_pt, final_pred_pt)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_pt, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Readmission', '30-day Readmission'], \n",
    "            yticklabels=['No Readmission', '30-day Readmission'])\n",
    "plt.title('Confusion Matrix - Optimized PyTorch Tabular')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix (PyTorch Tabular):\")\n",
    "print(f\"True Negatives: {cm_pt[0,0]}\")\n",
    "print(f\"False Positives: {cm_pt[0,1]}\")\n",
    "print(f\"False Negatives: {cm_pt[1,0]}\")\n",
    "print(f\"True Positives: {cm_pt[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The results have slightly worse accuracy than in XGBOOST method. However, the higher recall makes it relatively safer to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare XGBoost and PyTorch Tabular results\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON: XGBoost vs PyTorch Tabular\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# print(\"\\nCross-Validation Results (4-fold accuracy scores):\")\n",
    "# print(f\"XGBoost CV Accuracy:         {final_cv_scores.mean():.3f} ± {final_cv_scores.std():.3f}\")\n",
    "# print(f\"PyTorch Tabular CV Accuracy: {np.mean(final_cv_scores_pt):.3f} ± {np.std(final_cv_scores_pt):.3f}\")\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"{'Metric':<15} {'XGBoost':<10} {'PyTorch Tabular':<15}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Accuracy':<15} {test_accuracy:.3f}      {final_accuracy_pt:.3f}\")\n",
    "print(f\"{'Precision':<15} {test_precision:.3f}      {final_prec_pt:.3f}\")\n",
    "print(f\"{'Recall':<15} {test_recall:.3f}      {final_rec_pt:.3f}\")\n",
    "print(f\"{'F1':<15} {test_f1:.3f}      {final_f1_pt:.3f}\")\n",
    "print(f\"{'ROC-AUC':<15} {test_auc:.3f}      {final_auc_pt:.3f}\")\n",
    "\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "print(f\"\\nXGBoost:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "print(f\"\\nPyTorch Tabular:\")\n",
    "for key, value in study_pt.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While the accuracy is good, its low recall might make the model unsuitable for deployment yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model interpretation\n",
    "* We will use SHAP, a model-agnostic technique to analyze feature contributions the the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis for XGBoost Model\n",
    "print(\"Performing SHAP analysis on the optimized XGBoost model...\")\n",
    "\n",
    "# Get the XGBoost classifier from the best pipeline\n",
    "xgb_model = best_pipeline.named_steps['classifier']\n",
    "\n",
    "# Use a sample of training data for SHAP (to speed up computation)\n",
    "sample_size = 5000\n",
    "sample_indices = np.random.choice(X_test.shape[0], size=min(sample_size, X_test.shape[0]), replace=False)\n",
    "X_sample = X_test.iloc[sample_indices]\n",
    "\n",
    "# Create SHAP explainer for XGBoost\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "print(f\"SHAP values computed for {len(X_sample)} samples\")\n",
    "\n",
    "# Summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "plt.title('SHAP Feature Importance - XGBoost Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_sample, show=False)\n",
    "plt.title('SHAP Summary Plot - XGBoost Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance ranking\n",
    "feature_importance = np.abs(shap_values).mean(0)\n",
    "feature_names = X_sample.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features (SHAP):\")\n",
    "print(importance_df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class ModelWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        # Explicitly set estimator type\n",
    "        self._estimator_type = \"classifier\"\n",
    "        # Set classes_ attribute to indicate this is a binary classifier\n",
    "        self.classes_ = np.array([0, 1])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Get probability predictions for neg_log_loss scoring\n",
    "        preds = self.model.predict(X)\n",
    "        # Return probabilities for both classes\n",
    "        # prob_0 = preds[\"target_0_probability\"].values\n",
    "        # prob_1 = preds[\"target_1_probability\"].values\n",
    "        # return np.column_stack([prob_0, prob_1])\n",
    "\n",
    "        return preds[\"target_1_probability\"].values\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Required by sklearn but we don't need to do anything\n",
    "        # Set classes_ based on unique values in y if needed\n",
    "        self.classes_ = np.unique(y)\n",
    "        self._estimator_type = \"classifier\"\n",
    "        return self\n",
    "\n",
    "def custom_log_loss(y_true, y_pred_proba):\n",
    "    return -log_loss(y_true, y_pred_proba)\n",
    "\n",
    "custom_scorer = make_scorer(custom_log_loss, response_method='predict', greater_is_better=True)\n",
    "\n",
    "# Create the wrapper\n",
    "wrapped_model = ModelWrapper(final_model_pt)\n",
    "\n",
    "result = permutation_importance(wrapped_model, test_df_pt, y_test_pt, scoring='accuracy', n_repeats=20, random_state=1803)\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': train_df_pt.columns,\n",
    "    'importance': result.importances_mean\n",
    "}).sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.head(15).plot(kind='barh', x='feature', y='importance', figsize=(12, 8), legend=False)\n",
    "plt.title('Permutation Feature Importance - PyTorch Tabular Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_xgboost = permutation_importance(xgb_model, X_test, y_test, scoring='accuracy', n_repeats=10, random_state=1803)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df_xgboost = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': result_xgboost.importances_mean\n",
    "}).sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df_xgboost.head(15).plot(kind='barh', x='feature', y='importance', figsize=(12, 8), legend=False)\n",
    "plt.title('Permutation Feature Importance - XGBoost Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As of now we will not be implementing SHAP on the Pytorch Model as I am encountering compatibility issues between SHAP and Pytorch Tabular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the best PyTorch Tabular model\n",
    "# print(\"Reloading the best PyTorch Tabular model...\")\n",
    "\n",
    "# # Load the saved model\n",
    "# final_model_pt = TabularModel.load_model(\"best_pytorch_tabular_model\")\n",
    "\n",
    "# print(\"Model reloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_pt_float = train_df_pt.astype(np.float32)\n",
    "# # keep the feature order you trained with\n",
    "# FEATURE_NAMES = train_df_pt_float.columns.tolist()\n",
    "\n",
    "# def pytorch_tabular_predict_proba(X):\n",
    "#     \"\"\"\n",
    "#     SHAP wrapper for PyTorch-Tabular.\n",
    "#     Ensures the incoming object is a pandas.DataFrame\n",
    "#     with the correct column names and dtypes.\n",
    "#     Returns the positive-class probability reshaped for SHAP.\n",
    "#     \"\"\"\n",
    "\n",
    "#     pred_proba = final_model_pt.predict(X)\n",
    "\n",
    "#     # keep only P(y=1) and give SHAP a 2-D array\n",
    "#     return pred_proba[\"target_1_probability\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "# # ───  A.  Summarise the background  ────────────────────────────────────────────\n",
    "\n",
    "# dense_bg   = shap.kmeans(train_df_pt_float, k=100)             # DenseData (not callable)\n",
    "# masker     = shap.maskers.Independent(dense_bg.data)    # <-- make it callable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ───  B.  Build the explainer  ────────────────────────────────────────────────\n",
    "# explainer_pt = shap.Explainer(\n",
    "#     pytorch_tabular_predict_proba,  # returns *logits*, see previous answer\n",
    "#     masker,\n",
    "#     link=shap.links.logit,                   # tell SHAP what the wrapper outputs\n",
    "#     algorithm=\"permutation\"         # same default the auto-chooser would pick\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ───  C.  Explain a subset  ───────────────────────────────────────────────────\n",
    "\n",
    "# sample_size     = 500\n",
    "# sample_indices  = np.random.choice(len(train_df_pt_float),\n",
    "#                                    size=min(sample_size, len(train_df_pt_float)),\n",
    "#                                    replace=False)\n",
    "\n",
    "# X_sample_pt     = train_df_pt_float.iloc[sample_indices]\n",
    "# shap_values_pt = explainer_pt(X_sample_pt, max_evals=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap.summary_plot(shap_values_pt.values, features=X_sample_pt,\n",
    "#                   feature_names=X_sample_pt.columns, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature importance ranking for PyTorch Tabular\n",
    "# feature_importance_pt = np.abs(shap_values_pt).mean(0)\n",
    "# feature_names_pt = X_sample_pt.columns\n",
    "# importance_df_pt = pd.DataFrame({\n",
    "#     'feature': feature_names_pt,\n",
    "#     'importance': feature_importance_pt.ravel()\n",
    "# }).sort_values('importance', ascending=False)\n",
    "\n",
    "# print(\"\\nTop 60 Most Important Features (SHAP - PyTorch Tabular):\")\n",
    "# importance_df_pt.head(60).to_csv('pytorch_tabular_shap_importance.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diabetes-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
