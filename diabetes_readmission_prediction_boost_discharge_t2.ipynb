{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, accuracy_score,\n",
    "                             roc_auc_score, confusion_matrix, ConfusionMatrixDisplay)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pdb\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 1. Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (101766, 50)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>patient_nbr</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>...</th>\n",
       "      <th>citoglipton</th>\n",
       "      <th>insulin</th>\n",
       "      <th>glyburide-metformin</th>\n",
       "      <th>glipizide-metformin</th>\n",
       "      <th>glimepiride-pioglitazone</th>\n",
       "      <th>metformin-rosiglitazone</th>\n",
       "      <th>metformin-pioglitazone</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>readmitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2278392</td>\n",
       "      <td>8222157</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[0-10)</td>\n",
       "      <td>?</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149190</td>\n",
       "      <td>55629189</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[10-20)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64410</td>\n",
       "      <td>86047875</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Female</td>\n",
       "      <td>[20-30)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500364</td>\n",
       "      <td>82442376</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[30-40)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16680</td>\n",
       "      <td>42519267</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   encounter_id  patient_nbr             race  gender      age weight  \\\n",
       "0       2278392      8222157        Caucasian  Female   [0-10)      ?   \n",
       "1        149190     55629189        Caucasian  Female  [10-20)      ?   \n",
       "2         64410     86047875  AfricanAmerican  Female  [20-30)      ?   \n",
       "3        500364     82442376        Caucasian    Male  [30-40)      ?   \n",
       "4         16680     42519267        Caucasian    Male  [40-50)      ?   \n",
       "\n",
       "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
       "0                  6                        25                    1   \n",
       "1                  1                         1                    7   \n",
       "2                  1                         1                    7   \n",
       "3                  1                         1                    7   \n",
       "4                  1                         1                    7   \n",
       "\n",
       "   time_in_hospital  ... citoglipton insulin  glyburide-metformin  \\\n",
       "0                 1  ...          No      No                   No   \n",
       "1                 3  ...          No      Up                   No   \n",
       "2                 2  ...          No      No                   No   \n",
       "3                 2  ...          No      Up                   No   \n",
       "4                 1  ...          No  Steady                   No   \n",
       "\n",
       "   glipizide-metformin  glimepiride-pioglitazone  metformin-rosiglitazone  \\\n",
       "0                   No                        No                       No   \n",
       "1                   No                        No                       No   \n",
       "2                   No                        No                       No   \n",
       "3                   No                        No                       No   \n",
       "4                   No                        No                       No   \n",
       "\n",
       "   metformin-pioglitazone  change diabetesMed readmitted  \n",
       "0                      No      No          No         NO  \n",
       "1                      No      Ch         Yes        >30  \n",
       "2                      No      No         Yes         NO  \n",
       "3                      No      Ch         Yes         NO  \n",
       "4                      No      Ch         Yes         NO  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "DATA_DIR = Path('/home/arjay55/code/datasets/diabetes+130-us+hospitals+for+years+1999-2008')  # change if files are elsewhere\n",
    "df = pd.read_csv(DATA_DIR / 'diabetic_data.csv')\n",
    "ids_map = pd.read_csv(DATA_DIR / 'IDS_mapping.csv')\n",
    "print(f'Data shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical/Object columns:\n",
      "Count: 37\n",
      "['race', 'gender', 'age', 'weight', 'payer_code', 'medical_specialty', 'diag_1', 'diag_2', 'diag_3', 'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone', 'change', 'diabetesMed', 'readmitted']\n",
      "\n",
      "Integer columns:\n",
      "Count: 13\n",
      "['encounter_id', 'patient_nbr', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses']\n",
      "\n",
      "Total columns analyzed: 50\n",
      "DataFrame shape: (101766, 50)\n"
     ]
    }
   ],
   "source": [
    "# Print columns by data type\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "integer_cols = df.select_dtypes(include=['int64', 'int32']).columns\n",
    "\n",
    "print(\"Categorical/Object columns:\")\n",
    "print(f\"Count: {len(categorical_cols)}\")\n",
    "print(categorical_cols.tolist())\n",
    "\n",
    "print(\"\\nInteger columns:\")\n",
    "print(f\"Count: {len(integer_cols)}\")\n",
    "print(integer_cols.tolist())\n",
    "\n",
    "print(f\"\\nTotal columns analyzed: {len(categorical_cols) + len(integer_cols)}\")\n",
    "print(f\"DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# convert ['encounter_id', 'patient_nbr', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id'] to category\n",
    "categorical_cols = ['encounter_id', 'patient_nbr', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n",
    "df[categorical_cols] = df[categorical_cols].astype('category')\n",
    "\n",
    "categorical_cols_rest = df.select_dtypes(include=['object', 'category']).columns\n",
    "# convert rest of the categorical columns to category\n",
    "df[categorical_cols_rest] = df[categorical_cols_rest].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "df_pt = df.copy() # for Pytorch Tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_med_change(x):\n",
    "    \"\"\"\n",
    "    Simple ordinal encoder for medication‐change flags. No translates to zero as there is no drug. \n",
    "    Down can have the value of 1 as as the probability of relatively lower dosage than is more likely., 2 for steady meaning the drugs are normal,\n",
    "    3 for up as the probability of relatively higher dosage than is more likely.\n",
    "    \n",
    "    Maps:\n",
    "      \"No\"     → 0.0\n",
    "      \"Down\"   → 1.0\n",
    "      \"Steady\" → 2.0\n",
    "      \"Up\"     → 3.0\n",
    "    \n",
    "    Anything else → np.nan\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"no\":      0.0,\n",
    "        \"down\":    1.0,\n",
    "        \"steady\":  2.0,\n",
    "        \"up\":      3.0,\n",
    "    }\n",
    "    # normalize to lower‐case string, then lookup\n",
    "    return mapping.get(str(x).strip().lower(), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied medication change encoding to 23 columns\n",
      "Sample encoded values:\n",
      "  metformin repaglinide nateglinide chlorpropamide glimepiride\n",
      "0       0.0         0.0         0.0            0.0         0.0\n",
      "1       0.0         0.0         0.0            0.0         0.0\n",
      "2       0.0         0.0         0.0            0.0         0.0\n",
      "3       0.0         0.0         0.0            0.0         0.0\n",
      "4       0.0         0.0         0.0            0.0         0.0\n"
     ]
    }
   ],
   "source": [
    "# Apply medication change encoding to all medication columns\n",
    "medication_cols = [\n",
    "    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
    "    'examide', 'citoglipton', 'insulin', 'glyburide-metformin', \n",
    "    'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', \n",
    "    'metformin-pioglitazone'\n",
    "]\n",
    "\n",
    "for col in medication_cols:\n",
    "    df[col] = df[col].apply(encode_med_change)\n",
    "\n",
    "print(f\"Applied medication change encoding to {len(medication_cols)} columns\")\n",
    "print(\"Sample encoded values:\")\n",
    "print(df[medication_cols[:5]].head())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop weight as 97% have missing weights and drop impossible genders\n",
    "df = df[df['gender'] != 'Unknown/Invalid'].copy()\n",
    "df.drop(columns=['weight'], inplace=True)\n",
    "freq = df[\"patient_nbr\"].value_counts(normalize=True) # Calculate frequency of each patient. More frequent patients are more likely to have chronic conditions.\n",
    "df[\"patient_freq\"] = df[\"patient_nbr\"].map(freq)\n",
    "\n",
    "# Drop patient_nbr as it is not useful for modeling anymore\n",
    "df.drop(columns=['patient_nbr'], inplace=True)\n",
    "\n",
    "# Replace '?' with 'Unknown'\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "df[categorical_cols] = df[categorical_cols].replace('?', 'Unknown')\n",
    "\n",
    "# Remove encounters with discharge disposition indicating death/hospice\n",
    "hospice_codes = [11, 19, 20, 21]\n",
    "df = df[~df['discharge_disposition_id'].isin(hospice_codes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compute proportions\n",
    "freq = df['discharge_disposition_id'].value_counts(normalize=True)\n",
    "\n",
    "# 3. Select “major” IDs (≥1% of all records)\n",
    "major_ids = set(freq[freq >= 0.01].index)\n",
    "\n",
    "# 4. Map to reduced categories\n",
    "def bucket_disp(x):\n",
    "    return x if x in major_ids else 'Other'\n",
    "\n",
    "df['disch_reduced'] = df['discharge_disposition_id'].apply(bucket_disp)\n",
    "df_pt['disch_reduced'] = df_pt['discharge_disposition_id'].apply(bucket_disp)\n",
    "\n",
    "# 5. Drop original column\n",
    "df.drop(columns=['discharge_disposition_id'], inplace=True)\n",
    "# 6. Dummify the reduced column\n",
    "df = pd.get_dummies(df, columns=['disch_reduced'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 2.1 Map admission/disposition/source IDs\n",
    "  * Translates IDs to descriptions for easier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After applying mapping with converted keys:\n",
      "admission_type_id\n",
      "Other    100111\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create mapping for admission_type_id only (since that's what we have)\n",
    "def build_mapping_from_df(df_map):\n",
    "    # Remove any rows with NaN values\n",
    "    df_clean = df_map.dropna()\n",
    "    return dict(zip(df_clean['admission_type_id'], df_clean['description']))\n",
    "\n",
    "# Build the admission type mapping\n",
    "admission_type_mapping = build_mapping_from_df(ids_map)\n",
    "\n",
    "original_dtype = df['admission_type_id'].dtype\n",
    "\n",
    "# Create new mapping with converted keys\n",
    "if original_dtype in ['int64', 'int32', 'float64']:\n",
    "    # Convert string keys to numeric\n",
    "    admission_type_mapping_fixed = {\n",
    "        int(k): v for k, v in admission_type_mapping.items() \n",
    "        if k.isdigit()\n",
    "    }\n",
    "else:\n",
    "    # Keep as strings\n",
    "    admission_type_mapping_fixed = admission_type_mapping\n",
    "\n",
    "# Apply the mapping\n",
    "df['admission_type_id'] = df['admission_type_id'].map(admission_type_mapping_fixed).fillna('Other')\n",
    "df_pt['admission_type_id'] = df_pt['admission_type_id'].map(admission_type_mapping_fixed).fillna('Other')\n",
    "print(\"After applying mapping with converted keys:\")\n",
    "print(df['admission_type_id'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 2.2 Aggregate ICD‑9 diagnosis codes\n",
    "* First if statement are focused on internal, coronary and diabetic diseases, which could have comorbidities with each other, and thus we choose to make this detailed.\n",
    "* Other diseases are grouped, as they can have of less influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def diag_category(icd):\n",
    "    try:\n",
    "        icd = str(icd)\n",
    "        code = icd.split('.')[0]  # take 3‑digit root\n",
    "        if code.startswith('V') or code.startswith('E'):\n",
    "            return 'Other'\n",
    "        code = int(code)\n",
    "    except:\n",
    "        return 'Other'\n",
    "    if 390 <= code <= 459 or code == 785 or 460 <= code <= 519 or code == 786 or 520 <= code <= 579 or code == 787 or 250 <= code <= 251:\n",
    "        return f'icd_{code}'  # will result to very sparse categories\n",
    "    if 800 <= code <= 999:\n",
    "        return 'Injury'\n",
    "    if 710 <= code <= 739:\n",
    "        return 'Musculoskeletal'\n",
    "    if 140 <= code <= 239:\n",
    "        return 'Neoplasms'\n",
    "    if 580 <= code <= 629 or code == 788:\n",
    "        return 'Genitourinary'\n",
    "    return 'Other'\n",
    "\n",
    "for col in ['diag_1', 'diag_2', 'diag_3']:\n",
    "    df[f'{col}_cat'] = df[col].apply(diag_category)\n",
    "    df_pt[f'{col}_cat'] = df_pt[col].apply(diag_category)\n",
    "\n",
    "df.drop(columns=['diag_1','diag_2','diag_3'], inplace=True)\n",
    "df_pt.drop(columns=['diag_1','diag_2','diag_3'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encounter_id has no relevance in the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(columns=['encounter_id'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 3. Train‑test split & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_name(col_name):\n",
    "    \"\"\"Clean column names by removing special characters that XGBoost doesn't allow\"\"\"\n",
    "    return str(col_name).replace('[', '_').replace(']', '_').replace('<', '_lt_').replace('>', '_gt_').replace(',', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (70077, 53) Pos rate: 0.113\n",
      "Test size: (30034, 53) Pos rate: 0.113\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y = (df['readmitted'] == '<30').astype(int)\n",
    "X = df.drop(columns=['readmitted'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( #stratified sampling\n",
    "    X, y, test_size=0.3, stratify=y, random_state=1803)\n",
    "\n",
    "print('Train size:', X_train.shape, 'Pos rate:', y_train.mean().round(3))\n",
    "print('Test size:', X_test.shape, 'Pos rate:', y_test.mean().round(3))\n",
    "\n",
    "# Fix column names to remove special characters that XGBoost doesn't allow\n",
    "X_train.columns = [clean_column_name(col) for col in X_train.columns]\n",
    "X_test.columns = [clean_column_name(col) for col in X_test.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 3.1 Balance training set by random oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced class counts: readmitted\n",
      "0    62127\n",
      "1    62127\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "maj = train[train['readmitted']==0]\n",
    "minu = train[train['readmitted']==1]\n",
    "minu_upsampled = resample(minu, replace=True, n_samples=len(maj), random_state=1803)\n",
    "train_bal = pd.concat([maj, minu_upsampled])\n",
    "X_train_bal = train_bal.drop(columns=['readmitted'])\n",
    "y_train_bal = train_bal['readmitted']\n",
    "print('Balanced class counts:', y_train_bal.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 3.2 One‑hot encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering for baseline runs completed.\n",
      "Creating dummy variables for training and test sets for pipeline use...\n",
      "Categorical columns to encode: ['race', 'gender', 'age', 'admission_type_id', 'admission_source_id', 'payer_code', 'medical_specialty', 'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone', 'change', 'diabetesMed', 'diag_1_cat', 'diag_2_cat', 'diag_3_cat']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# categorical features (\"object\" dtype) are dummified, meaning they are converted to one-hot encoded columns.\n",
    "cat_feats = X_train_bal.select_dtypes(include=['object','category']).columns\n",
    "X_train_bal_enc = pd.get_dummies(X_train_bal, columns=cat_feats, drop_first=True) # reduce collinearity\n",
    "X_test_enc = pd.get_dummies(X_test, columns=cat_feats, drop_first=True) # reduce collinearity\n",
    "X_train_bal_enc, X_test_enc = X_train_bal_enc.align(X_test_enc, join='left', axis=1, fill_value=0)\n",
    "\n",
    "## haircut for it to be compatible with XGBoost\n",
    "X_train_bal_enc.columns = [clean_column_name(col) for col in X_train_bal_enc.columns]\n",
    "X_test_enc.columns = [clean_column_name(col) for col in X_test_enc.columns]\n",
    "\n",
    "# Apply standard scaling to numeric features\n",
    "num_feats = X_train_bal_enc.select_dtypes(include=['int64','float64']).columns\n",
    "scaler = StandardScaler()\n",
    "X_train_bal_enc[num_feats] = scaler.fit_transform(X_train_bal_enc[num_feats])\n",
    "X_test_enc[num_feats] = scaler.transform(X_test_enc[num_feats])\n",
    "\n",
    "print(\"Feature engineering for baseline runs completed.\")\n",
    "# Dummify categorical variables for X_train and X_test\n",
    "\n",
    "print(\"Creating dummy variables for training and test sets for pipeline use...\")\n",
    "\n",
    "# Get categorical columns\n",
    "cat_cols = X_train.select_dtypes(include=['object','category']).columns\n",
    "print(f\"Categorical columns to encode: {list(cat_cols)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Dummify X_train and X_test\n",
    "X_train = pd.get_dummies(X_train, columns=cat_feats, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=cat_feats, drop_first=True)\n",
    "\n",
    "scaler = StandardScaler() # normalize!\n",
    "X_train[num_feats] = scaler.fit_transform(X_train[num_feats])\n",
    "X_test[num_feats] = scaler.transform(X_test[num_feats])\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Clean column names for XGBoost compatibility\n",
    "X_train.columns = [clean_column_name(col) for col in X_train.columns]\n",
    "X_test.columns = [clean_column_name(col) for col in X_test.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n",
      "Training Logistic Regression...\n",
      "Training Random Forest...\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBClassifier\">?<span>Documentation for XGBClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='logloss',\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=6, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"Initializing models...\")\n",
    "logreg = LogisticRegression(max_iter=1000, solver='lbfgs', random_state=1803)\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=1803)\n",
    "xgb = XGBClassifier(n_estimators=100, max_depth=6, eval_metric='logloss',\n",
    "                    use_label_encoder=False, verbosity=0, random_state=1803)\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "logreg.fit(X_train_bal_enc, y_train_bal)\n",
    "print(\"Training Random Forest...\")\n",
    "rf.fit(X_train_bal_enc, y_train_bal)\n",
    "print(\"Training XGBoost...\")\n",
    "xgb.fit(X_train_bal_enc, y_train_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression  Precision: 0.221 Recall: 0.587 F1: 0.321 ROC-AUC: 0.723 Accuracy: 0.719\n",
      "Random Forest        Precision: 0.204 Recall: 0.733 F1: 0.320 ROC-AUC: 0.739 Accuracy: 0.646\n",
      "XGBoost              Precision: 0.236 Recall: 0.732 F1: 0.358 ROC-AUC: 0.779 Accuracy: 0.701\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def eval_model(name, model):\n",
    "    y_pred = model.predict(X_test_enc)\n",
    "    y_prob = model.predict_proba(X_test_enc)[:,1]\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name:20} Precision: {prec:.3f} Recall: {rec:.3f} F1: {f1:.3f} ROC-AUC: {auc:.3f} Accuracy: {acc:.3f}\")\n",
    "    return y_pred\n",
    "\n",
    "preds = {}\n",
    "preds['Logistic'] = eval_model('Logistic Regression', logreg)\n",
    "preds['RandomForest'] = eval_model('Random Forest', rf)\n",
    "preds['XGBoost'] = eval_model('XGBoost', xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Results show suboptimal performance. The class imbalance is significant, due to small positivity rate of 0.113.\n",
    "* We will proceed with XGBOOST due to its versatility and a go-to algorithm for tabular data.\n",
    "* We will use optuna as a hyperparameter tuning tool, a generic hyperparameter tuning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 21:30:20,411] A new study created in memory with name: no-name-303ac3d7-b314-48be-99be-2cb2055f9c1f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna hyperparameter optimization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6f3b050ddb49e99ec79a54cc7f8bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-22 21:30:49,973] Trial 0 finished with value: 0.8788618353628325 and parameters: {'n_estimators': 164, 'max_depth': 15, 'learning_rate': 0.21191694474081635, 'subsample': 0.6904437683143655, 'colsample_bytree': 0.8618698714066619, 'reg_alpha': 0.3962959879812257, 'reg_lambda': 0.909399760281288}. Best is trial 0 with value: 0.8788618353628325.\n",
      "[I 2025-06-22 21:31:41,297] Trial 1 finished with value: 0.8775632607363538 and parameters: {'n_estimators': 309, 'max_depth': 13, 'learning_rate': 0.013593797341396345, 'subsample': 0.7425593088977969, 'colsample_bytree': 0.9273841019764437, 'reg_alpha': 0.3435138821068924, 'reg_lambda': 0.29783633190422765}. Best is trial 0 with value: 0.8788618353628325.\n",
      "[I 2025-06-22 21:32:55,970] Trial 2 finished with value: 0.8778058349434573 and parameters: {'n_estimators': 513, 'max_depth': 14, 'learning_rate': 0.21957825505770912, 'subsample': 0.7201632969976054, 'colsample_bytree': 0.9336080058865639, 'reg_alpha': 0.3499472600676322, 'reg_lambda': 0.7594278254340586}. Best is trial 0 with value: 0.8788618353628325.\n",
      "[I 2025-06-22 21:34:01,911] Trial 3 finished with value: 0.8842844191212099 and parameters: {'n_estimators': 515, 'max_depth': 15, 'learning_rate': 0.028353130845057178, 'subsample': 0.984520264849488, 'colsample_bytree': 0.9105489577462976, 'reg_alpha': 0.12721485786224984, 'reg_lambda': 0.7978656505759597}. Best is trial 3 with value: 0.8842844191212099.\n",
      "[I 2025-06-22 21:35:07,915] Trial 4 finished with value: 0.8751801762000865 and parameters: {'n_estimators': 749, 'max_depth': 10, 'learning_rate': 0.23128584962270882, 'subsample': 0.6818656119096016, 'colsample_bytree': 0.6921535098577161, 'reg_alpha': 0.018891700698558056, 'reg_lambda': 0.25721971961996315}. Best is trial 3 with value: 0.8842844191212099.\n",
      "[I 2025-06-22 21:36:41,010] Trial 5 finished with value: 0.8795610623404049 and parameters: {'n_estimators': 833, 'max_depth': 13, 'learning_rate': 0.16903962762671607, 'subsample': 0.8158125074749883, 'colsample_bytree': 0.6922208239113067, 'reg_alpha': 0.11413592422807495, 'reg_lambda': 0.09848931049641729}. Best is trial 3 with value: 0.8842844191212099.\n",
      "[I 2025-06-22 21:37:02,342] Trial 6 finished with value: 0.8697290658465786 and parameters: {'n_estimators': 127, 'max_depth': 11, 'learning_rate': 0.020909310732237153, 'subsample': 0.9092701277591608, 'colsample_bytree': 0.6384582780698584, 'reg_alpha': 0.23565540666025375, 'reg_lambda': 0.706858283215527}. Best is trial 3 with value: 0.8842844191212099.\n",
      "[I 2025-06-22 21:37:16,520] Trial 7 finished with value: 0.8553591334040198 and parameters: {'n_estimators': 70, 'max_depth': 10, 'learning_rate': 0.012338270935489783, 'subsample': 0.618139498652425, 'colsample_bytree': 0.7011708628994365, 'reg_alpha': 0.41662751075490023, 'reg_lambda': 0.16610777062300053}. Best is trial 3 with value: 0.8842844191212099.\n",
      "[I 2025-06-22 21:37:49,256] Trial 8 finished with value: 0.8863393227861415 and parameters: {'n_estimators': 411, 'max_depth': 7, 'learning_rate': 0.07015713392361016, 'subsample': 0.7125142649939546, 'colsample_bytree': 0.6191493413509745, 'reg_alpha': 0.5611301769940157, 'reg_lambda': 0.8338901604236937}. Best is trial 8 with value: 0.8863393227861415.\n",
      "[I 2025-06-22 21:38:15,136] Trial 9 finished with value: 0.874323987381215 and parameters: {'n_estimators': 185, 'max_depth': 10, 'learning_rate': 0.023019794591372034, 'subsample': 0.6267112252907016, 'colsample_bytree': 0.6437855132387006, 'reg_alpha': 0.28410130131758327, 'reg_lambda': 0.7034910458321948}. Best is trial 8 with value: 0.8863393227861415.\n",
      "[I 2025-06-22 21:38:56,734] Trial 10 finished with value: 0.8868530409645262 and parameters: {'n_estimators': 676, 'max_depth': 5, 'learning_rate': 0.07562608757805468, 'subsample': 0.8249576109988048, 'colsample_bytree': 0.794589304136872, 'reg_alpha': 0.7206851458567882, 'reg_lambda': 0.514034945934564}. Best is trial 10 with value: 0.8868530409645262.\n",
      "[I 2025-06-22 21:39:38,440] Trial 11 finished with value: 0.8869814574769571 and parameters: {'n_estimators': 677, 'max_depth': 5, 'learning_rate': 0.0788904684410345, 'subsample': 0.8244269635087204, 'colsample_bytree': 0.7905921798819306, 'reg_alpha': 0.7023795461588158, 'reg_lambda': 0.4852405347816829}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:40:15,699] Trial 12 finished with value: 0.8849265831343973 and parameters: {'n_estimators': 691, 'max_depth': 3, 'learning_rate': 0.07438144108494087, 'subsample': 0.8345305529193011, 'colsample_bytree': 0.776054124920724, 'reg_alpha': 0.8253013029029614, 'reg_lambda': 0.5138789218405333}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:41:12,170] Trial 13 finished with value: 0.8847410857383544 and parameters: {'n_estimators': 964, 'max_depth': 5, 'learning_rate': 0.10453154769236642, 'subsample': 0.876402569735921, 'colsample_bytree': 0.800934758511383, 'reg_alpha': 0.733610393217959, 'reg_lambda': 0.47530647797741393}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:42:00,212] Trial 14 finished with value: 0.886396387379547 and parameters: {'n_estimators': 648, 'max_depth': 7, 'learning_rate': 0.03885129968497195, 'subsample': 0.77473007814537, 'colsample_bytree': 0.797591483001457, 'reg_alpha': 0.9572248679415827, 'reg_lambda': 0.5232314928851255}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:42:44,324] Trial 15 finished with value: 0.8866389974251048 and parameters: {'n_estimators': 877, 'max_depth': 3, 'learning_rate': 0.11845017232109509, 'subsample': 0.9103839838497843, 'colsample_bytree': 0.851295467842979, 'reg_alpha': 0.6149833086672943, 'reg_lambda': 0.39274953107674837}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:43:25,917] Trial 16 finished with value: 0.8868958467401734 and parameters: {'n_estimators': 612, 'max_depth': 6, 'learning_rate': 0.05202040188176045, 'subsample': 0.8544268683105353, 'colsample_bytree': 0.9953972510979988, 'reg_alpha': 0.7358474300287209, 'reg_lambda': 0.6149173907996321}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:43:59,717] Trial 17 finished with value: 0.8859397533428156 and parameters: {'n_estimators': 425, 'max_depth': 7, 'learning_rate': 0.05000999721400084, 'subsample': 0.9813024466460374, 'colsample_bytree': 0.9904363502653082, 'reg_alpha': 0.9858854541755734, 'reg_lambda': 0.6274010524637549}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:44:40,128] Trial 18 finished with value: 0.8854688368675262 and parameters: {'n_estimators': 595, 'max_depth': 5, 'learning_rate': 0.04019942949228837, 'subsample': 0.8682356813056531, 'colsample_bytree': 0.7323247784347524, 'reg_alpha': 0.8637116743560078, 'reg_lambda': 0.6171151963154202}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:45:43,562] Trial 19 finished with value: 0.8802174713246753 and parameters: {'n_estimators': 782, 'max_depth': 8, 'learning_rate': 0.12384057391859445, 'subsample': 0.78388820476412, 'colsample_bytree': 0.9784030669017161, 'reg_alpha': 0.6455571192349892, 'reg_lambda': 0.34262621992435643}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:46:19,079] Trial 20 finished with value: 0.8848837765442399 and parameters: {'n_estimators': 585, 'max_depth': 4, 'learning_rate': 0.054638915750222915, 'subsample': 0.9372904571883017, 'colsample_bytree': 0.8499055713062095, 'reg_alpha': 0.49201263975588944, 'reg_lambda': 0.9633872088830937}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:47:01,510] Trial 21 finished with value: 0.886367837163617 and parameters: {'n_estimators': 700, 'max_depth': 5, 'learning_rate': 0.08135324935666785, 'subsample': 0.8510219684041945, 'colsample_bytree': 0.7725168680580958, 'reg_alpha': 0.7270543169043029, 'reg_lambda': 0.45020598614077023}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:48:04,643] Trial 22 finished with value: 0.8842844435565196 and parameters: {'n_estimators': 912, 'max_depth': 6, 'learning_rate': 0.08783385889025172, 'subsample': 0.7977391199140204, 'colsample_bytree': 0.8251960068762962, 'reg_alpha': 0.7359514443823221, 'reg_lambda': 0.5935401079811738}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:48:44,621] Trial 23 finished with value: 0.8835281275502318 and parameters: {'n_estimators': 611, 'max_depth': 4, 'learning_rate': 0.04007629788191647, 'subsample': 0.7582618378108879, 'colsample_bytree': 0.7424280220718726, 'reg_alpha': 0.853683574933276, 'reg_lambda': 0.5642718402854868}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:49:19,255] Trial 24 finished with value: 0.8862536827269858 and parameters: {'n_estimators': 452, 'max_depth': 6, 'learning_rate': 0.060746869866041484, 'subsample': 0.8197115230011077, 'colsample_bytree': 0.8667403148284748, 'reg_alpha': 0.6452603363695112, 'reg_lambda': 0.4353771423749956}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:50:20,379] Trial 25 finished with value: 0.886710328981372 and parameters: {'n_estimators': 749, 'max_depth': 8, 'learning_rate': 0.029991303479153127, 'subsample': 0.8894844185097053, 'colsample_bytree': 0.8996085972507258, 'reg_alpha': 0.7911517545329033, 'reg_lambda': 0.6820146112032281}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:50:44,999] Trial 26 finished with value: 0.8859397460122227 and parameters: {'n_estimators': 345, 'max_depth': 4, 'learning_rate': 0.15863008073836504, 'subsample': 0.9497250325360523, 'colsample_bytree': 0.7471773030397912, 'reg_alpha': 0.9152330373995761, 'reg_lambda': 0.3186553981500847}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:51:41,615] Trial 27 finished with value: 0.8848837765442399 and parameters: {'n_estimators': 841, 'max_depth': 6, 'learning_rate': 0.0945068392932286, 'subsample': 0.8444133077109786, 'colsample_bytree': 0.8144121553377354, 'reg_alpha': 0.5421628293853854, 'reg_lambda': 0.20614768593845412}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:52:27,229] Trial 28 finished with value: 0.885811322983709 and parameters: {'n_estimators': 546, 'max_depth': 8, 'learning_rate': 0.04934150431094014, 'subsample': 0.8070573452768027, 'colsample_bytree': 0.9585593957962344, 'reg_alpha': 0.6867579626946112, 'reg_lambda': 0.01716701559046152}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:53:04,216] Trial 29 finished with value: 0.884027576322224 and parameters: {'n_estimators': 640, 'max_depth': 3, 'learning_rate': 0.06683291895897618, 'subsample': 0.6757775270019001, 'colsample_bytree': 0.8793996193902625, 'reg_alpha': 0.4685366787385074, 'reg_lambda': 0.3988856472248588}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:53:49,038] Trial 30 finished with value: 0.8831428372874226 and parameters: {'n_estimators': 692, 'max_depth': 5, 'learning_rate': 0.1719973099088609, 'subsample': 0.8589830256713066, 'colsample_bytree': 0.8403076401440516, 'reg_alpha': 0.5856200859499733, 'reg_lambda': 0.8690377295547025}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:54:50,075] Trial 31 finished with value: 0.8862822345719363 and parameters: {'n_estimators': 761, 'max_depth': 8, 'learning_rate': 0.02686603075542769, 'subsample': 0.8926856869922177, 'colsample_bytree': 0.8931277904495265, 'reg_alpha': 0.7806793109924924, 'reg_lambda': 0.6837459081557262}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:56:01,048] Trial 32 finished with value: 0.8856828885520508 and parameters: {'n_estimators': 793, 'max_depth': 9, 'learning_rate': 0.017391789724781857, 'subsample': 0.8898673110952237, 'colsample_bytree': 0.9489346097044224, 'reg_alpha': 0.7977628144456438, 'reg_lambda': 0.5467965220917564}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:56:51,557] Trial 33 finished with value: 0.88596828971207 and parameters: {'n_estimators': 725, 'max_depth': 6, 'learning_rate': 0.0316496961762384, 'subsample': 0.9350215187466553, 'colsample_bytree': 0.9063879991804568, 'reg_alpha': 0.6758019599499955, 'reg_lambda': 0.6441772973472155}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:57:32,029] Trial 34 finished with value: 0.8858255834304882 and parameters: {'n_estimators': 489, 'max_depth': 7, 'learning_rate': 0.03699035293646096, 'subsample': 0.8337550747103325, 'colsample_bytree': 0.9283004643709183, 'reg_alpha': 0.9092992258803263, 'reg_lambda': 0.802852390467218}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:58:32,845] Trial 35 finished with value: 0.8754655635134301 and parameters: {'n_estimators': 557, 'max_depth': 9, 'learning_rate': 0.010441928979229704, 'subsample': 0.728209045920233, 'colsample_bytree': 0.7786651907483704, 'reg_alpha': 0.77590893302834, 'reg_lambda': 0.7481960148665998}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 21:59:12,558] Trial 36 finished with value: 0.8850978355593575 and parameters: {'n_estimators': 633, 'max_depth': 4, 'learning_rate': 0.049366384711549645, 'subsample': 0.7772549575365253, 'colsample_bytree': 0.9651002853401222, 'reg_alpha': 0.8811753100349089, 'reg_lambda': 0.49751816884928146}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:00:08,397] Trial 37 finished with value: 0.8828146327952873 and parameters: {'n_estimators': 836, 'max_depth': 6, 'learning_rate': 0.13082548576259406, 'subsample': 0.905795423559683, 'colsample_bytree': 0.8270301087807678, 'reg_alpha': 0.7159383552728539, 'reg_lambda': 0.7518441366766663}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:01:38,494] Trial 38 finished with value: 0.8758080390409785 and parameters: {'n_estimators': 668, 'max_depth': 12, 'learning_rate': 0.2616233207427174, 'subsample': 0.7496671075750906, 'colsample_bytree': 0.7153590709152791, 'reg_alpha': 0.7907120235482765, 'reg_lambda': 0.6661032643388988}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:02:58,527] Trial 39 finished with value: 0.8860967168131352 and parameters: {'n_estimators': 963, 'max_depth': 8, 'learning_rate': 0.016305011927539716, 'subsample': 0.8743688938271463, 'colsample_bytree': 0.9167797054321134, 'reg_alpha': 0.539115934444824, 'reg_lambda': 0.5780279374258588}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:04:39,758] Trial 40 finished with value: 0.8847838817398774 and parameters: {'n_estimators': 743, 'max_depth': 15, 'learning_rate': 0.030536391107363448, 'subsample': 0.824004752303597, 'colsample_bytree': 0.9973858176246426, 'reg_alpha': 0.4093138022231332, 'reg_lambda': 0.3688512703675432}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:05:24,857] Trial 41 finished with value: 0.8859825591184626 and parameters: {'n_estimators': 867, 'max_depth': 3, 'learning_rate': 0.10656462660303742, 'subsample': 0.9183932550390539, 'colsample_bytree': 0.8639293895486834, 'reg_alpha': 0.6061910769109573, 'reg_lambda': 0.25712131584238174}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:06:21,044] Trial 42 finished with value: 0.8864962870709713 and parameters: {'n_estimators': 923, 'max_depth': 5, 'learning_rate': 0.06188920789075048, 'subsample': 0.900263553423067, 'colsample_bytree': 0.84422382523798, 'reg_alpha': 0.6330690110947224, 'reg_lambda': 0.3987452596071429}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:07:05,277] Trial 43 finished with value: 0.8861252507388587 and parameters: {'n_estimators': 795, 'max_depth': 4, 'learning_rate': 0.12578510406178206, 'subsample': 0.9562246552301191, 'colsample_bytree': 0.7903937591702044, 'reg_alpha': 0.6802352586788117, 'reg_lambda': 0.4397784734184448}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:07:35,887] Trial 44 finished with value: 0.8828289013871697 and parameters: {'n_estimators': 508, 'max_depth': 3, 'learning_rate': 0.07700267710685028, 'subsample': 0.9221038393674993, 'colsample_bytree': 0.7600720638755148, 'reg_alpha': 0.7494384004412198, 'reg_lambda': 0.2679005917756513}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:08:59,138] Trial 45 finished with value: 0.8813305387809869 and parameters: {'n_estimators': 894, 'max_depth': 9, 'learning_rate': 0.10129958636865866, 'subsample': 0.8799777666145988, 'colsample_bytree': 0.893026063949847, 'reg_alpha': 0.8166324143864004, 'reg_lambda': 0.502940192237281}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:09:54,425] Trial 46 finished with value: 0.8807597437915418 and parameters: {'n_estimators': 725, 'max_depth': 7, 'learning_rate': 0.15498867288416743, 'subsample': 0.8550997494159482, 'colsample_bytree': 0.8111069812625319, 'reg_alpha': 0.6080001937021589, 'reg_lambda': 0.7216272960574592}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:10:43,640] Trial 47 finished with value: 0.8821582026337484 and parameters: {'n_estimators': 811, 'max_depth': 5, 'learning_rate': 0.1900297695534942, 'subsample': 0.9959845507976743, 'colsample_bytree': 0.9397901853029637, 'reg_alpha': 0.690348761708883, 'reg_lambda': 0.5490339115548784}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:11:03,839] Trial 48 finished with value: 0.8575423981946801 and parameters: {'n_estimators': 230, 'max_depth': 4, 'learning_rate': 0.043722010903741504, 'subsample': 0.803618865302106, 'colsample_bytree': 0.8803506639975089, 'reg_alpha': 0.45403279423835424, 'reg_lambda': 0.6140484787969548}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:12:41,550] Trial 49 finished with value: 0.885668618331148 and parameters: {'n_estimators': 996, 'max_depth': 11, 'learning_rate': 0.025563100626478295, 'subsample': 0.8402628537111606, 'colsample_bytree': 0.8328324498949098, 'reg_alpha': 0.3605246914421423, 'reg_lambda': 0.47399808733111715}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:13:35,274] Trial 50 finished with value: 0.8865961663996376 and parameters: {'n_estimators': 676, 'max_depth': 7, 'learning_rate': 0.03470738583981415, 'subsample': 0.7887578669278086, 'colsample_bytree': 0.6681208824551398, 'reg_alpha': 0.055973986246218965, 'reg_lambda': 0.3793092047106588}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:14:30,539] Trial 51 finished with value: 0.8859968260813244 and parameters: {'n_estimators': 673, 'max_depth': 7, 'learning_rate': 0.021920888530852542, 'subsample': 0.8251753965941543, 'colsample_bytree': 0.6641133457975317, 'reg_alpha': 0.11558771087124656, 'reg_lambda': 0.3887950669880062}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:15:14,674] Trial 52 finished with value: 0.8859682937846216 and parameters: {'n_estimators': 611, 'max_depth': 6, 'learning_rate': 0.03547612972768845, 'subsample': 0.7870611717586715, 'colsample_bytree': 0.6059352860161269, 'reg_alpha': 0.1463137187777525, 'reg_lambda': 0.303893464833375}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:15:52,693] Trial 53 finished with value: 0.8863107684976598 and parameters: {'n_estimators': 569, 'max_depth': 5, 'learning_rate': 0.05760713538338499, 'subsample': 0.7640347360975596, 'colsample_bytree': 0.6735698899950858, 'reg_alpha': 0.1921484948758235, 'reg_lambda': 0.3522793402990987}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:16:49,111] Trial 54 finished with value: 0.8852547982151667 and parameters: {'n_estimators': 709, 'max_depth': 8, 'learning_rate': 0.06828909541373605, 'subsample': 0.8632077751848459, 'colsample_bytree': 0.7218773017823722, 'reg_alpha': 0.8314874591234518, 'reg_lambda': 0.4311048581732686}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:17:42,541] Trial 55 finished with value: 0.8843129717806707 and parameters: {'n_estimators': 765, 'max_depth': 7, 'learning_rate': 0.08877126326443142, 'subsample': 0.794131620696601, 'colsample_bytree': 0.637923115696472, 'reg_alpha': 0.30491345074709103, 'reg_lambda': 0.5239237693136808}. Best is trial 11 with value: 0.8869814574769571.\n",
      "[I 2025-06-22 22:18:28,956] Trial 56 finished with value: 0.8863535791603689 and parameters: {'n_estimators': 667, 'max_depth': 6, 'learning_rate': 0.04385210566635281, 'subsample': 0.813665168558095, 'colsample_bytree': 0.8541617743428225, 'reg_alpha': 0.010423480766881543, 'reg_lambda': 0.588992417611129}. Best is trial 11 with value: 0.8869814574769571.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "\n",
    "# Create pipeline with proper order: preprocess -> balance -> model (with Optuna parameters)\n",
    "def create_model_pipeline(trial=None):\n",
    "    \n",
    "    # If trial is provided, optimize hyperparameters\n",
    "    if trial is not None:\n",
    "        # Optuna hyperparameter suggestions for XGBoost\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 15)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "        subsample = trial.suggest_float('subsample', 0.6, 1.0)\n",
    "        colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "        reg_alpha = trial.suggest_float('reg_alpha', 0, 1.0)\n",
    "        reg_lambda = trial.suggest_float('reg_lambda', 0, 1.0)\n",
    "        \n",
    "        classifier = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            eval_metric='logloss',\n",
    "            use_label_encoder=False,\n",
    "            verbosity=0,\n",
    "            random_state=1803,\n",
    "            n_jobs=10  # Use all available cores\n",
    "        )\n",
    "    else:\n",
    "        # Use default/best known parameters for XGBoost\n",
    "        classifier = XGBClassifier(\n",
    "            n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            eval_metric='logloss', use_label_encoder=False,\n",
    "            verbosity=0, random_state=1803\n",
    "        )\n",
    "    rng = np.random.default_rng()\n",
    "    pipeline = ImbPipeline([ \n",
    "        ('balancer', SMOTE(random_state=rng.integers(2**16))), # pipeline performs oversampling per each fold, avoiding data leakage\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    # Create pipeline with trial parameters\n",
    "    pipeline = create_model_pipeline(trial)\n",
    "    \n",
    "    # Cross-validation with proper data handling\n",
    "\n",
    "    cv_scores = cross_val_score(\n",
    "        pipeline, X_train, y_train, \n",
    "        cv=4,  \n",
    "        scoring='accuracy',\n",
    "        n_jobs=1  # Reduced to prevent system overload\n",
    "    )\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "# Run Optuna optimization\n",
    "print(\"Starting Optuna hyperparameter optimization...\")\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=1803)\n",
    ")\n",
    "\n",
    "# Optimize with progress callback\n",
    "# def callback(study, trial):\n",
    "#     if trial.number % 5 == 0:\n",
    "#         print(f\"Trial {trial.number}: Best value = {study.best_value:.4f}\")\n",
    "\n",
    "study.optimize(\n",
    "    objective, \n",
    "    n_trials=100,\n",
    "    # callbacks=[callback],\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "# Print optimization results\n",
    "print(f\"\\nOptimization completed!\")\n",
    "print(f\"Best parameters: {study.best_params}\")\n",
    "print(f\"Best CV accuracy score: {study.best_value:.4f}\")\n",
    "\n",
    "# Create final pipeline with best parameters\n",
    "print(\"\\nTraining final model with best parameters...\")\n",
    "best_pipeline = create_model_pipeline()\n",
    "\n",
    "# Update the classifier with best parameters from Optuna\n",
    "best_pipeline.named_steps['classifier'].set_params(**study.best_params)\n",
    "\n",
    "# Cross-validation with best parameters\n",
    "final_cv_scores = cross_val_score(best_pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Final CV accuracy score: {final_cv_scores.mean():.3f} ± {final_cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining final model with best parameters...\")\n",
    "best_pipeline = create_model_pipeline()\n",
    "\n",
    "# Update the classifier with best parameters from Optuna\n",
    "best_pipeline.named_steps['classifier'].set_params(**study.best_params,random_state=1803)\n",
    "\n",
    "# Cross-validation with best parameters\n",
    "final_cv_scores = cross_val_score(best_pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Final CV accuracy score: {final_cv_scores.mean():.3f} ± {final_cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model on all training data\n",
    "best_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_pred_test = best_pipeline.predict(X_test)\n",
    "y_prob_test = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test)\n",
    "test_recall = recall_score(y_test, y_pred_test)\n",
    "test_auc = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "print(f\"\\nFinal Test Performance:\")\n",
    "print(f\"Accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"F1: {test_f1:.3f}\")\n",
    "print(f\"Precision: {test_precision:.3f}\")\n",
    "print(f\"Recall: {test_recall:.3f}\")\n",
    "print(f\"ROC-AUC: {test_auc:.3f}\")\n",
    "\n",
    "# Save the best model for later use\n",
    "best_rf_optimized = best_pipeline.named_steps['classifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy was well achieved. However the lack of data for readmissions resulted in highly skewed result plus some other modeling imperfections. The model is not yet safe for deployment. A higher recall is better. Class weighting that biases on readmission rates will be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Readmission', '30-day Readmission'], \n",
    "            yticklabels=['No Readmission', '30-day Readmission'])\n",
    "plt.title('Confusion Matrix - Optimized Random Forest')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"True Negatives: {cm[0,0]}\")\n",
    "print(f\"False Positives: {cm[0,1]}\")\n",
    "print(f\"False Negatives: {cm[1,0]}\")\n",
    "print(f\"True Positives: {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tabular Implementation\n",
    "\n",
    "Now we'll implement the same training pipeline using PyTorch Tabular with neural networks instead of XGBoost. Pytorch Tabular aims to implement suitable neural network architectures for tabular data with ease of use in using other popular frameworks, like Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch Tabular\n",
    "import torch\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"PyTorch Tabular imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch Tabular\n",
    "# We'll use the same train/test split as XGBoost but with different preprocessing\n",
    "print(\"Preparing data for PyTorch Tabular...\")\n",
    "\n",
    "# Start with the original X_train, X_test, y_train, y_test\n",
    "# Reset from the original data before one-hot encoding\n",
    "y = (df_pt['readmitted'] == '<30').astype(int)\n",
    "X = df.drop(columns=['readmitted'])\n",
    "\n",
    "X_train_pt, X_test_pt, y_train_pt, y_test_pt = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=1803)\n",
    "\n",
    "print('PyTorch Tabular - Train size:', X_train_pt.shape, 'Pos rate:', y_train_pt.mean().round(3))\n",
    "print('PyTorch Tabular - Test size:', X_test_pt.shape, 'Pos rate:', y_test_pt.mean().round(3))\n",
    "\n",
    "# Clean column names\n",
    "X_train_pt.columns = [clean_column_name(col) for col in X_train_pt.columns]\n",
    "X_test_pt.columns = [clean_column_name(col) for col in X_test_pt.columns]\n",
    "\n",
    "print(\"Data prepared for PyTorch Tabular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance training set by random oversampling (same as XGBoost)\n",
    "print(\"Balancing training data...\")\n",
    "\n",
    "maj_pt = train_df_pt[train_df_pt['target']==0]\n",
    "minu_pt = train_df_pt[train_df_pt['target']==1]\n",
    "minu_upsampled_pt = resample(minu_pt, replace=True, n_samples=len(maj_pt), random_state=1803)\n",
    "train_bal_df_pt = pd.concat([maj_pt, minu_upsampled_pt])\n",
    "\n",
    "print('Balanced class counts for PyTorch Tabular:', train_bal_df_pt['target'].value_counts())\n",
    "print('Balanced training set shape:', train_bal_df_pt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and numerical columns for PyTorch Tabular\n",
    "categorical_cols_pt = [col for col in X_train_pt.columns if X_train_pt[col].dtype == 'object' or X_train_pt[col].dtype.name == 'category']\n",
    "numerical_cols_pt = [col for col in X_train_pt.columns if X_train_pt[col].dtype != 'object' and X_train_pt[col].dtype.name != 'category']\n",
    "\n",
    "print(f\"Categorical columns ({len(categorical_cols_pt)}): {categorical_cols_pt[:5]}...\")\n",
    "print(f\"Numerical columns ({len(numerical_cols_pt)}): {numerical_cols_pt[:5]}...\")\n",
    "\n",
    "# Make sure we have the correct categoricals\n",
    "categorical_cols_pt = []\n",
    "numerical_cols_pt = []\n",
    "\n",
    "for col in X_train_pt.columns:\n",
    "    if X_train_pt[col].dtype == 'object' or str(X_train_pt[col].dtype) == 'category':\n",
    "        categorical_cols_pt.append(col)\n",
    "    else:\n",
    "        numerical_cols_pt.append(col)\n",
    "\n",
    "# Dummify categorical columns for PyTorch Tabular\n",
    "# X_train_pt = pd.get_dummies(X_train_pt, columns=categorical_cols_pt, drop_first=True)\n",
    "# X_test_pt = pd.get_dummies(X_test_pt, columns=categorical_cols_pt, drop_first=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Normalize numerical columns for PyTorch Tabular\n",
    "X_train_pt[numerical_cols_pt] = scaler.fit_transform(X_train_pt[numerical_cols_pt])\n",
    "X_test_pt[numerical_cols_pt] = scaler.transform(X_test_pt[numerical_cols_pt])\n",
    "X_train_pt, X_test_pt = X_train_pt.align(X_test_pt, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Add target column to create complete datasets\n",
    "train_df_pt = X_train_pt.copy()\n",
    "train_df_pt['target'] = y_train_pt.values\n",
    "\n",
    "test_df_pt = X_test_pt.copy()\n",
    "test_df_pt['target'] = y_test_pt.values\n",
    "\n",
    "# Configure PyTorch Tabular Data Config\n",
    "data_config = DataConfig(\n",
    "    target=['target'],  # Target column\n",
    "    continuous_cols=numerical_cols_pt,  # Numerical columns\n",
    "    categorical_cols=categorical_cols_pt,  # Categorical columns\n",
    "    normalize_continuous_features=True,  # Similar to StandardScaler\n",
    ")\n",
    "\n",
    "print(\"PyTorch Tabular Data Config created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will use CategoryEmbeddingModelConfig, where categorical data are transformed into high dimensional embeddings.\n",
    "* We will go through the process of trying the model incrementally prior to proceeding to hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline PyTorch Tabular model configuration\n",
    "model_config = CategoryEmbeddingModelConfig(\n",
    "    task=\"classification\",\n",
    "    layers=\"128-64-32\",  # Neural network architecture\n",
    "    activation=\"ReLU\",\n",
    "    dropout=0.1,\n",
    "    use_batch_norm=True,  # Correct parameter name\n",
    "    learning_rate=1e-3,\n",
    "    seed=1803,\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024,\n",
    "    max_epochs=50,\n",
    "    early_stopping=\"valid_loss\",\n",
    "    early_stopping_patience=10,\n",
    "    checkpoints=None,  # Disable checkpoints to avoid loading issues\n",
    "    load_best=False,   # Don't try to load best model\n",
    "    progress_bar=\"none\",  # Disable progress bar for cleaner output\n",
    "    auto_lr_find=False,  # We'll set learning rate manually\n",
    "    auto_select_gpus=torch.cuda.is_available(),\n",
    "    seed=1803,\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "print(\"PyTorch Tabular configurations created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline PyTorch Tabular model\n",
    "print(\"Training baseline PyTorch Tabular model...\")\n",
    "\n",
    "baseline_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "baseline_model.fit(train=train_bal_df_pt, validation=test_df_pt)\n",
    "\n",
    "print(\"Baseline PyTorch Tabular model training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline PyTorch Tabular model\n",
    "print(\"Evaluating baseline PyTorch Tabular model...\")\n",
    "\n",
    "# Make predictions\n",
    "baseline_pred_proba = baseline_model.predict(test_df_pt)\n",
    "print(\"Prediction output shape:\", baseline_pred_proba.shape)\n",
    "print(\"Prediction output columns:\", baseline_pred_proba.columns.tolist())\n",
    "\n",
    "# Get prediction probabilities - use the correct column name\n",
    "if '1' in baseline_pred_proba.columns:\n",
    "    baseline_proba_values = baseline_pred_proba['1'].values\n",
    "elif '1_probability' in baseline_pred_proba.columns:\n",
    "    baseline_proba_values = baseline_pred_proba['1_probability'].values\n",
    "else:\n",
    "    # Try the first numeric column after the identifier columns\n",
    "    prob_cols = [col for col in baseline_pred_proba.columns if col not in ['patient_nbr', 'target']]\n",
    "    baseline_proba_values = baseline_pred_proba[prob_cols[0]].values\n",
    "\n",
    "baseline_pred = (baseline_proba_values > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "baseline_prec = precision_score(y_test_pt, baseline_pred)\n",
    "baseline_rec = recall_score(y_test_pt, baseline_pred)\n",
    "baseline_f1 = f1_score(y_test_pt, baseline_pred)\n",
    "baseline_auc = roc_auc_score(y_test_pt, baseline_proba_values)\n",
    "\n",
    "print(f\"Baseline PyTorch Tabular Performance:\")\n",
    "print(f\"Precision: {baseline_prec:.3f}\")\n",
    "print(f\"Recall: {baseline_rec:.3f}\")\n",
    "print(f\"F1: {baseline_f1:.3f}\")\n",
    "print(f\"ROC-AUC: {baseline_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Results are suboptimal, will proceed to k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement 4-fold cross-validation for PyTorch Tabular\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def pytorch_tabular_cv(X_data, y_data, n_folds=4, model_params=None):\n",
    "    \"\"\"\n",
    "    Perform cross-validation for PyTorch Tabular model\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=1803)\n",
    "    cv_scores = []\n",
    "    \n",
    "    # Combine X and y for easier handling\n",
    "    full_data = X_data.copy()\n",
    "    full_data['target'] = y_data.values\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_data, y_data)):\n",
    "        print(f\"Training fold {fold + 1}/{n_folds}...\")\n",
    "        \n",
    "        # Split data\n",
    "        train_fold = full_data.iloc[train_idx]\n",
    "        val_fold = full_data.iloc[val_idx]\n",
    "        \n",
    "        # Balance training fold\n",
    "        maj_fold = train_fold[train_fold['target']==0]\n",
    "        minu_fold = train_fold[train_fold['target']==1]\n",
    "        minu_upsampled_fold = resample(minu_fold, replace=True, n_samples=len(maj_fold), random_state=1803+fold)\n",
    "        train_fold_balanced = pd.concat([maj_fold, minu_upsampled_fold])\n",
    "        \n",
    "        # Create model configuration\n",
    "        if model_params is None:\n",
    "            fold_model_config = CategoryEmbeddingModelConfig(\n",
    "                task=\"classification\",\n",
    "                layers=\"128-64-32\",\n",
    "                activation=\"ReLU\", \n",
    "                dropout=0.1,\n",
    "                use_batch_norm=True,  # Fixed parameter name\n",
    "                learning_rate=1e-3,\n",
    "                seed=1803+fold,\n",
    "            )\n",
    "        else:\n",
    "            fold_model_config = CategoryEmbeddingModelConfig(\n",
    "                task=\"classification\",\n",
    "                **model_params,\n",
    "                seed=1803+fold,\n",
    "            )\n",
    "        \n",
    "        fold_trainer_config = TrainerConfig(\n",
    "            batch_size=1024,\n",
    "            max_epochs=30,  # Reduced for CV\n",
    "            early_stopping=\"valid_loss\",\n",
    "            early_stopping_patience=5,\n",
    "            checkpoints=None,  # Don't save checkpoints for CV\n",
    "            load_best=True,   # Don't try to load best model\n",
    "            progress_bar=\"none\",\n",
    "            auto_lr_find=False,\n",
    "            auto_select_gpus=torch.cuda.is_available(),\n",
    "            seed=1803+fold,\n",
    "        )\n",
    "        \n",
    "        # Create and train model\n",
    "        fold_model = TabularModel(\n",
    "            data_config=data_config,\n",
    "            model_config=fold_model_config,\n",
    "            optimizer_config=optimizer_config,\n",
    "            trainer_config=fold_trainer_config,\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            fold_model.fit(train=train_fold_balanced, validation=val_fold)\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            val_pred_proba = fold_model.predict(val_fold)\n",
    "            # Use correct column name for predictions\n",
    "            val_proba_values = val_pred_proba['target_1_probability'].values\n",
    "            val_pred = (val_proba_values > 0.5).astype(int)\n",
    "            val_accuracy = accuracy_score(val_fold['target'], val_pred)\n",
    "            cv_scores.append(val_accuracy)\n",
    "            \n",
    "            print(f\"Fold {fold + 1} accuracy score: {val_accuracy:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fold {fold + 1}: {e}\")\n",
    "            cv_scores.append(0.0)  # Add poor score for failed fold\n",
    "    \n",
    "    return cv_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform 4-fold cross-validation with baseline model\n",
    "print(\"Performing 4-fold cross-validation with PyTorch Tabular...\")\n",
    "cv_scores_pt = pytorch_tabular_cv(X_train_pt, y_train_pt, n_folds=4)\n",
    "\n",
    "print(f\"\\nPyTorch Tabular CV Results:\")\n",
    "print(f\"Mean Accuracy: {np.mean(cv_scores_pt):.3f} ± {np.std(cv_scores_pt):.3f}\")\n",
    "print(f\"Individual fold scores: {[f'{score:.3f}' for score in cv_scores_pt]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracies are suboptimal. Hopefully hyperparameter tuning will enhance this.\n",
    "* In this case, layer depths, activation, batch sizes, etc will be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for PyTorch Tabular using Optuna\n",
    "import optuna\n",
    "\n",
    "def pytorch_tabular_objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for PyTorch Tabular hyperparameter optimization\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    layers_depth = trial.suggest_int('layers_depth', 2, 4)\n",
    "    layer_size = trial.suggest_categorical('layer_size', [64, 128, 256, 512])\n",
    "    \n",
    "    # Create layer string\n",
    "    if layers_depth == 2:\n",
    "        layers = f\"{layer_size}-{layer_size//2}\"\n",
    "    elif layers_depth == 3:\n",
    "        layers = f\"{layer_size}-{layer_size//2}-{layer_size//4}\"\n",
    "    else:  # layers_depth == 4\n",
    "        layers = f\"{layer_size}-{layer_size//2}-{layer_size//4}-{layer_size//8}\"\n",
    "    \n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [512, 1024, 2048])\n",
    "    activation = trial.suggest_categorical('activation', ['ReLU', 'GELU', 'LeakyReLU'])\n",
    "    \n",
    "    # Model parameters\n",
    "    model_params = {\n",
    "        'layers': layers,\n",
    "        'activation': activation,\n",
    "        'dropout': dropout,\n",
    "        'use_batch_norm': True,  # Fixed parameter name\n",
    "        'learning_rate': learning_rate,\n",
    "    }\n",
    "    \n",
    "    # Use trainer config with suggested batch size\n",
    "    global data_config, optimizer_config\n",
    "    \n",
    "    # Perform cross-validation with reduced folds for speed\n",
    "    cv_scores = []\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1803)  # Reduced to 3 folds for speed\n",
    "    \n",
    "    # Combine X and y for easier handling\n",
    "    full_data = X_train_pt.copy()\n",
    "    full_data['target'] = y_train_pt.values\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_pt, y_train_pt)):\n",
    "        # Split data\n",
    "        train_fold = full_data.iloc[train_idx]\n",
    "        val_fold = full_data.iloc[val_idx]\n",
    "        \n",
    "        # Balance training fold\n",
    "        maj_fold = train_fold[train_fold['target']==0]\n",
    "        minu_fold = train_fold[train_fold['target']==1]\n",
    "        minu_upsampled_fold = resample(minu_fold, replace=True, n_samples=len(maj_fold))\n",
    "        train_fold_balanced = pd.concat([maj_fold, minu_upsampled_fold])\n",
    "        \n",
    "        # Create model configuration\n",
    "        fold_model_config = CategoryEmbeddingModelConfig(\n",
    "            task=\"classification\",\n",
    "            **model_params,\n",
    "            seed=1803+fold,\n",
    "        )\n",
    "        \n",
    "        fold_trainer_config = TrainerConfig(\n",
    "            batch_size=batch_size,\n",
    "            max_epochs=20,  # Reduced for hyperparameter tuning\n",
    "            early_stopping=\"valid_loss\",\n",
    "            early_stopping_patience=3,\n",
    "            checkpoints=None,\n",
    "            load_best=False,\n",
    "            progress_bar=\"none\",\n",
    "            auto_lr_find=False,\n",
    "            auto_select_gpus=torch.cuda.is_available(),\n",
    "            seed=1803+fold,\n",
    "        )\n",
    "        \n",
    "        # Create and train model\n",
    "        fold_model = TabularModel(\n",
    "            data_config=data_config,\n",
    "            model_config=fold_model_config,\n",
    "            optimizer_config=optimizer_config,\n",
    "            trainer_config=fold_trainer_config,\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Train model\n",
    "            fold_model.fit(train=train_fold_balanced, validation=val_fold)\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            val_pred_proba = fold_model.predict(val_fold)\n",
    "            val_proba_values = val_pred_proba['target_1_probability'].values\n",
    "            val_pred = (val_proba_values > 0.5).astype(int)\n",
    "            val_accuracy = accuracy_score(val_fold['target'], val_pred)\n",
    "            cv_scores.append(val_accuracy)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fold {fold}: {e}\")\n",
    "            return 0.0  # Return poor score for failed trials\n",
    "    \n",
    "    if len(cv_scores) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Run Optuna optimization for PyTorch Tabular (smaller trial for speed)\n",
    "print(\"Starting Optuna hyperparameter optimization for PyTorch Tabular...\")\n",
    "study_pt = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=1803)\n",
    ")\n",
    "\n",
    "study_pt.optimize(\n",
    "    pytorch_tabular_objective, \n",
    "    n_trials=50,  \n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "# Print optimization results\n",
    "print(f\"\\nPyTorch Tabular Optimization completed!\")\n",
    "print(f\"Best parameters: {study_pt.best_params}\")\n",
    "print(f\"Best CV accuracy score: {study_pt.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final optimized PyTorch Tabular model\n",
    "print(\"Training final optimized PyTorch Tabular model...\")\n",
    "\n",
    "# Extract best parameters\n",
    "best_params_pt = study_pt.best_params\n",
    "layers_depth = best_params_pt['layers_depth']\n",
    "layer_size = best_params_pt['layer_size']\n",
    "\n",
    "# Create layer string\n",
    "if layers_depth == 2:\n",
    "    layers = f\"{layer_size}-{layer_size//2}\"\n",
    "elif layers_depth == 3:\n",
    "    layers = f\"{layer_size}-{layer_size//2}-{layer_size//4}\"\n",
    "else:  # layers_depth == 4\n",
    "    layers = f\"{layer_size}-{layer_size//2}-{layer_size//4}-{layer_size//8}\"\n",
    "\n",
    "# Create final model configuration with best parameters\n",
    "final_model_config_pt = CategoryEmbeddingModelConfig(\n",
    "    task=\"classification\",\n",
    "    layers=layers,\n",
    "    activation=best_params_pt['activation'],\n",
    "    dropout=best_params_pt['dropout'],\n",
    "    use_batch_norm=True,\n",
    "    learning_rate=best_params_pt['learning_rate'],\n",
    "    seed=1803,\n",
    ")\n",
    "\n",
    "final_trainer_config_pt = TrainerConfig(\n",
    "    batch_size=best_params_pt['batch_size'],\n",
    "    max_epochs=50,  # Full epochs for final model\n",
    "    early_stopping=\"valid_loss\",\n",
    "    early_stopping_patience=5,\n",
    "    checkpoints=\"valid_loss\",\n",
    "    load_best=False,\n",
    "    progress_bar=\"none\",\n",
    "    auto_lr_find=False,\n",
    "    auto_select_gpus=torch.cuda.is_available(),\n",
    "    seed=1803,\n",
    ")\n",
    "\n",
    "# Create final model\n",
    "final_model_pt = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=final_model_config_pt,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=final_trainer_config_pt,\n",
    ")\n",
    "\n",
    "# Train final model\n",
    "final_model_pt.fit(train=train_bal_df_pt, validation=test_df_pt)\n",
    "\n",
    "print(\"Final PyTorch Tabular model training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cross-validation with optimized parameters\n",
    "# print(\"Performing final 4-fold cross-validation with optimized parameters...\")\n",
    "\n",
    "# # Extract best parameters for CV function\n",
    "# best_model_params = {\n",
    "#     'layers': layers,\n",
    "#     'activation': best_params_pt['activation'],\n",
    "#     'dropout': best_params_pt['dropout'],\n",
    "#     'use_batch_norm': True,\n",
    "#     'learning_rate': best_params_pt['learning_rate'],\n",
    "# }\n",
    "\n",
    "# # Perform final CV with optimized parameters\n",
    "# final_cv_scores_pt = pytorch_tabular_cv(X_train_pt, y_train_pt, n_folds=4, model_params=best_model_params)\n",
    "\n",
    "# print(f\"\\nFinal PyTorch Tabular CV Results (with optimization):\")\n",
    "# print(f\"Mean Accuracy: {np.mean(final_cv_scores_pt):.3f} ± {np.std(final_cv_scores_pt):.3f}\")\n",
    "# print(f\"Individual fold scores: {[f'{score:.3f}' for score in final_cv_scores_pt]}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating final model on test set...\")\n",
    "final_pred_proba_pt = final_model_pt.predict(test_df_pt)\n",
    "final_pred_pt = (final_pred_proba_pt['target_1_probability'].values > 0.5).astype(int)\n",
    "final_accuracy_pt = accuracy_score(y_test_pt, final_pred_pt)\n",
    "final_prec_pt = precision_score(y_test_pt, final_pred_pt)\n",
    "final_rec_pt = recall_score(y_test_pt, final_pred_pt)\n",
    "final_f1_pt = f1_score(y_test_pt, final_pred_pt)\n",
    "final_auc_pt = roc_auc_score(y_test_pt, final_pred_proba_pt['target_1_probability'].values)\n",
    "\n",
    "print(f\"\\nFinal PyTorch Tabular Test Performance:\")\n",
    "print(f\"Accuracy: {final_accuracy_pt:.3f}\")\n",
    "print(f\"Precision: {final_prec_pt:.3f}\")\n",
    "print(f\"Recall: {final_rec_pt:.3f}\")\n",
    "print(f\"F1: {final_f1_pt:.3f}\")\n",
    "print(f\"ROC-AUC: {final_auc_pt:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final PyTorch Tabular model\n",
    "print(\"Saving PyTorch Tabular model...\")\n",
    "\n",
    "# Save the model\n",
    "model_save_path = \"best_pytorch_tabular_model\"\n",
    "final_model_pt.save_model(model_save_path)\n",
    "\n",
    "# Also save the best hyperparameters\n",
    "with open(\"best_pytorch_tabular_params.txt\", \"w\") as f:\n",
    "    f.write(\"Best PyTorch Tabular Hyperparameters:\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\")\n",
    "    for key, value in study_pt.best_params.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "    f.write(f\"\\nBest CV Accuracy Score: {study_pt.best_value:.4f}\\n\")\n",
    "    f.write(f\"Final Test Accuracy Score: {final_accuracy_pt:.4f}\\n\")\n",
    "    f.write(f\"Final Test F1 Score: {final_f1_pt:.4f}\\n\")\n",
    "    f.write(f\"Final Test ROC-AUC: {final_auc_pt:.4f}\\n\")\n",
    "\n",
    "print(\"Model and parameters saved successfully\")\n",
    "\n",
    "# Create confusion matrix for PyTorch Tabular\n",
    "cm_pt = confusion_matrix(y_test_pt, final_pred_pt)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_pt, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Readmission', '30-day Readmission'], \n",
    "            yticklabels=['No Readmission', '30-day Readmission'])\n",
    "plt.title('Confusion Matrix - Optimized PyTorch Tabular')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix (PyTorch Tabular):\")\n",
    "print(f\"True Negatives: {cm_pt[0,0]}\")\n",
    "print(f\"False Positives: {cm_pt[0,1]}\")\n",
    "print(f\"False Negatives: {cm_pt[1,0]}\")\n",
    "print(f\"True Positives: {cm_pt[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The results have slightly worse accuracy than in XGBOOST method. However, the higher recall makes it relatively safer to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare XGBoost and PyTorch Tabular results\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON: XGBoost vs PyTorch Tabular\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# print(\"\\nCross-Validation Results (4-fold accuracy scores):\")\n",
    "# print(f\"XGBoost CV Accuracy:         {final_cv_scores.mean():.3f} ± {final_cv_scores.std():.3f}\")\n",
    "# print(f\"PyTorch Tabular CV Accuracy: {np.mean(final_cv_scores_pt):.3f} ± {np.std(final_cv_scores_pt):.3f}\")\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"{'Metric':<15} {'XGBoost':<10} {'PyTorch Tabular':<15}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Accuracy':<15} {test_accuracy:.3f}      {final_accuracy_pt:.3f}\")\n",
    "print(f\"{'Precision':<15} {test_precision:.3f}      {final_prec_pt:.3f}\")\n",
    "print(f\"{'Recall':<15} {test_recall:.3f}      {final_rec_pt:.3f}\")\n",
    "print(f\"{'F1':<15} {test_f1:.3f}      {final_f1_pt:.3f}\")\n",
    "print(f\"{'ROC-AUC':<15} {test_auc:.3f}      {final_auc_pt:.3f}\")\n",
    "\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "print(f\"\\nXGBoost:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "print(f\"\\nPyTorch Tabular:\")\n",
    "for key, value in study_pt.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While the accuracy is good, its low recall might make the model unsuitable for deployment yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model interpretation\n",
    "* We will use SHAP, a model-agnostic technique to analyze feature contributions the the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis for XGBoost Model\n",
    "print(\"Performing SHAP analysis on the optimized XGBoost model...\")\n",
    "\n",
    "# Get the XGBoost classifier from the best pipeline\n",
    "xgb_model = best_pipeline.named_steps['classifier']\n",
    "\n",
    "# Use a sample of training data for SHAP (to speed up computation)\n",
    "sample_size = 5000\n",
    "sample_indices = np.random.choice(X_train.shape[0], size=min(sample_size, X_train.shape[0]), replace=False)\n",
    "X_sample = X_train.iloc[sample_indices]\n",
    "\n",
    "# Create SHAP explainer for XGBoost\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "print(f\"SHAP values computed for {len(X_sample)} samples\")\n",
    "\n",
    "# Summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "plt.title('SHAP Feature Importance - XGBoost Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_sample, show=False)\n",
    "plt.title('SHAP Summary Plot - XGBoost Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance ranking\n",
    "feature_importance = np.abs(shap_values).mean(0)\n",
    "feature_names = X_sample.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features (SHAP):\")\n",
    "print(importance_df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the best PyTorch Tabular model\n",
    "# print(\"Reloading the best PyTorch Tabular model...\")\n",
    "\n",
    "# # Load the saved model\n",
    "# final_model_pt = TabularModel.load_model(\"best_pytorch_tabular_model\")\n",
    "\n",
    "# print(\"Model reloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_pt_float = train_df_pt.astype(np.float32)\n",
    "# keep the feature order you trained with\n",
    "FEATURE_NAMES = train_df_pt_float.columns.tolist()\n",
    "\n",
    "def pytorch_tabular_predict_proba(X):\n",
    "    \"\"\"\n",
    "    SHAP wrapper for PyTorch-Tabular.\n",
    "    Ensures the incoming object is a pandas.DataFrame\n",
    "    with the correct column names and dtypes.\n",
    "    Returns the positive-class probability reshaped for SHAP.\n",
    "    \"\"\"\n",
    "\n",
    "    pred_proba = final_model_pt.predict(X)\n",
    "\n",
    "    # keep only P(y=1) and give SHAP a 2-D array\n",
    "    return pred_proba[\"target_1_probability\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "# ───  A.  Summarise the background  ────────────────────────────────────────────\n",
    "\n",
    "dense_bg   = shap.kmeans(train_df_pt_float, k=100)             # DenseData (not callable)\n",
    "masker     = shap.maskers.Independent(dense_bg.data)    # <-- make it callable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───  B.  Build the explainer  ────────────────────────────────────────────────\n",
    "explainer_pt = shap.Explainer(\n",
    "    pytorch_tabular_predict_proba,  # returns *logits*, see previous answer\n",
    "    masker,\n",
    "    link=shap.links.logit,                   # tell SHAP what the wrapper outputs\n",
    "    algorithm=\"permutation\"         # same default the auto-chooser would pick\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───  C.  Explain a subset  ───────────────────────────────────────────────────\n",
    "\n",
    "sample_size     = 500\n",
    "sample_indices  = np.random.choice(len(train_df_pt_float),\n",
    "                                   size=min(sample_size, len(train_df_pt_float)),\n",
    "                                   replace=False)\n",
    "\n",
    "X_sample_pt     = train_df_pt_float.iloc[sample_indices]\n",
    "shap_values_pt = explainer_pt(X_sample_pt, max_evals=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_pt.values, features=X_sample_pt,\n",
    "                  feature_names=X_sample_pt.columns, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance ranking for PyTorch Tabular\n",
    "feature_importance_pt = np.abs(shap_values_pt).mean(0)\n",
    "feature_names_pt = X_sample_pt.columns\n",
    "importance_df_pt = pd.DataFrame({\n",
    "    'feature': feature_names_pt,\n",
    "    'importance': feature_importance_pt.ravel()\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 60 Most Important Features (SHAP - PyTorch Tabular):\")\n",
    "importance_df_pt.head(60).to_csv('pytorch_tabular_shap_importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_pt.columns.sort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diabetes-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
